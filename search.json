[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/post-with-code/road_seg.html",
    "href": "posts/post-with-code/road_seg.html",
    "title": "Road_segmantation",
    "section": "",
    "text": "This is a post with executable code.\n\nImports\n\n#Datasets\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n#Utils\nimport os\nimport cv2\nimport numpy as np\nimport torch\n\n#ConfusionMatrix\nimport numpy as np\n\n#Loss\nimport torch\nimport torch.nn.functional as F\n\n#Train\nimport os\nimport argparse\nimport torch\nimport torch.optim as optim\nimport time\nimport wandb\nfrom pathlib import Path\nfrom torchvision import models\n\n\n\nDataset\n\nclass KariRoadDataset(torch.utils.data.Dataset):\n    def __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\n\n    def __getitem__(self, idx):\n        img_file= self.img_files[idx].as_posix()\n        label_file = img_file.replace('images', 'labels')\n        img = cv2.imread(img_file)\n        label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n        img, label = self.transform(img, label)\n        return img, label, img_file\n\n    def __len__(self):\n        return len(self.img_files)\n    \nclass ImageAug:\n    def __init__(self, train):\n        if train:\n            self.aug = A.Compose([A.RandomCrop(256, 256),\n                                  A.HorizontalFlip(p=0.5),\n                                  A.ShiftScaleRotate(p=0.3),\n                                  A.RandomBrightnessContrast(p=0.3),\n                                  A.pytorch.transforms.ToTensorV2()])\n        else:\n            self.aug = ToTensorV2()\n\n    def __call__(self, img, label):\n        transformed = self.aug(image=img, mask=np.squeeze(label))\n        return transformed['image']/255.0, transformed['mask']\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\nKariRoadDatasetImageAugget_transforms\n\n\n\n init getitemlen\n\n\ndef __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\ninit ë©”ì†Œë“œëŠ” í´ë˜ìŠ¤ê°€ ì´ˆê¸°í™”ë  ë•Œ í•™ìŠµ ì—¬ë¶€ì— ë”°ë¼ ë°ì´í„°ì…‹ì˜ ë£¨íŠ¸ ê²½ë¡œì™€ë¥¼ ì„¤ì •í•˜ê³ ,\nì´ ì •ë³´ë“¤ì„ í´ë˜ìŠ¤ ë‚´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n\n\ndef __getitem__(self, idx):\n    img_file= self.img_files[idx].as_posix()\n    label_file = img_file.replace('images', 'labels')\n    img = cv2.imread(img_file)\n    label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n    img, label = self.transform(img, label)\n    return img, label, img_file\n\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n\n\n\n\ninitcall\n\n\ndef __init__(self, train):\n    if train:\n        self.aug = A.Compose([A.RandomCrop(256, 256),\n                                A.HorizontalFlip(p=0.5),\n                                A.ShiftScaleRotate(p=0.3),\n                                A.RandomBrightnessContrast(p=0.3),\n                                A.pytorch.transforms.ToTensorV2()])\n    else:\n        self.aug = ToTensorV2()\n\n\ndef __call__(self, img, label):\n    transformed = self.aug(image=img, mask=np.squeeze(label))\n    return transformed['image']/255.0, transformed['mask']\n\n\n\n\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\n\n\n\n\n\nì½”ë“œ\n\n---\ndef __init__(self, root, train=False):\n---\n\n\n\n\n\n\nê²°ê³¼\n\n---\në§¤ë²ˆ í´ë˜ìŠ¤ê°€ ì´ˆê¸°í™”ë  ë•Œë§ˆë‹¤.\n1) í•™ìŠµ ëª¨ë“œì— ë”°ë¥¸ ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œì„¤ì • \n2) class ë‚´ì—ì„œ ì‚¬ìš©í•  ì •ë³´ë“¤ì„ ì •ì˜\n---\n\n\n\n\n\nUtils\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\n\n\n make_color_label plot_image\n\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\ninit ë©”ì†Œë“œëŠ” í´ë˜ìŠ¤ê°€ ì´ˆê¸°í™”ë  ë•Œ í•™ìŠµ ì—¬ë¶€ì— ë”°ë¼ ë°ì´í„°ì…‹ì˜ ë£¨íŠ¸ ê²½ë¡œì™€ë¥¼ ì„¤ì •í•˜ê³ ,\nì´ ì •ë³´ë“¤ì„ í´ë˜ìŠ¤ ë‚´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\ninit ë©”ì†Œë“œëŠ” í´ë˜ìŠ¤ê°€ ì´ˆê¸°í™”ë  ë•Œ í•™ìŠµ ì—¬ë¶€ì— ë”°ë¼ ë°ì´í„°ì…‹ì˜ ë£¨íŠ¸ ê²½ë¡œì™€ë¥¼ ì„¤ì •í•˜ê³ ,\nì´ ì •ë³´ë“¤ì„ í´ë˜ìŠ¤ ë‚´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„í•©ë‹ˆë‹¤.\n\n\n\n\n\nConfusionMatrix\n\nclass ConfusionMatrix:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.confusion_matrix = np.zeros((num_classes, num_classes))\n        \n    def process_batch(self, preds, targets):\n        targets = targets.cpu().numpy().flatten()\n        preds = preds.argmax(1).cpu().numpy().flatten()\n        mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n        \n        confusion_mtx = np.bincount(\n                        self.num_classes * targets[mask].astype(int) + preds[mask],\n                        minlength=self.num_classes ** 2)\n        confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n        self.confusion_matrix += confusion_mtx\n        return confusion_mtx\n    \n    def print(self):\n        for i in range(self.num_classes):\n            print(f\"Class{i}:{self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n\n    def get_pix_acc(self):\n        return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n\n    def get_class_acc(self):\n        class_acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        return np.nanmean(class_acc),class_acc\n    \n    def get_iou(self):\n        divisor = self.confusion_matrix.sum(axis=1)\\\n                    + self.confusion_matrix.sum(axis=0)\\\n                    - np.diag(self.confusion_matrix)\n        iou = np.diag(self.confusion_matrix) / divisor\n        return iou\n    \n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\n \n\n\nConfusionMatrixì‚¬ìš©ì˜ˆì‹œ\n\n\n\n init process_batchprintget_pix_accget_class_accget_iouget_mean_iou\n\n\ndef __init__(self, num_classes):\n    self.num_classes = num_classes\n    self.confusion_matrix = np.zeros((num_classes, num_classes))\ninit ë©”ì†Œë“œëŠ” í´ë˜ìŠ¤ê°€ ì´ˆê¸°í™”ë  ë•Œ í´ë˜ìŠ¤ ë‚´ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„ë¬¼ì„ ì¤€ë¹„í•˜ëŠ” ë‹¨ê³„ì…ë‹ˆë‹¤.\nì˜ˆì‹œ ìƒí™©\n classì˜ ìˆ˜ë¥¼ 4ì´ë¼ê³  í–ˆì„ë•Œ, 0ìœ¼ë¡œ êµ¬ì„±ëœ 4by4í–‰ë ¬ì´ ì¤€ë¹„ëœë‹¤.\n\n\n\nâ€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€â€np.zeros((3,3))\n\n\n\n\ndef process_batch(self, preds, targets):\n    targets = targets.cpu().numpy().flatten()\n    preds = preds.argmax(1).cpu().numpy().flatten()\n    mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n    \n    confusion_mtx = np.bincount(\n                    self.num_classes * targets[mask].astype(int) + preds[mask],\n                    minlength=self.num_classes ** 2)\n    confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n    self.confusion_matrix += confusion_mtx\n[ì„¤ëª…]\n\n\n\nì´í•´ë¥¼ ë•ê¸°ìœ„í•´ ì‚¬ì§„ì˜ targetsê³¼ predsì„ process_batchì— ì ìš©í•´ë³´ë ¤ê³  í•œë‹¤.\n\n\nmaskëŠ” ì‚¬ì§„ì—ì„œ ì„¤ì •í•œ í´ë˜ìŠ¤(ì—¬ê¸°ì„œëŠ” 4ë¡œ ì„¤ì •í–ˆìœ¼ë‹ˆê¹ 0~3) ê°’ì¸ ê²ƒë§Œ ë‚¨ê¸°ê³ ,\nê·¸ ì™¸ì˜ ê°’ì€ ì œì™¸í•˜ëŠ” ì—­í• ì„ í•œë‹¤.\n\n\n\nì •ë‹µtraget.shape=(3,3) ì¶”ë¡ ê°’preds.shape=(3,3)ì„ flatenìœ¼ë¡œ ì¼ë ¬ë¡œ ë‚˜ì—´í•´ shapeì„ (9)ë¡œ ë§Œë“ ë‹¤.\n\n\n\n\n\n[í´ë˜ìŠ¤*ì •ë‹µ(9) + ì¶”ë¡ ê°’(9)]ì˜ ì‹ì„ ì´ìš©í•´ ì¸ë±ìŠ¤ë¥¼ ê³„ì‚°í•œë‹¤. ê²°ê³¼=&gt; [0, 6, 10, 13, 10, 5, 0, 15, 4]\n\n\n\n\n\n[0, 6, 10, 13, 10, 5, 0, 15, 4]ì—ì„œ 0ì€ 2ê°œ 1ì€ 0ê°œ 15ì€ 1ê°œ ì´ëŸ°ì‹ìœ¼ë¡œ bincountë¡œ ì›ì†Œê°’ì˜ ê°¯ìˆ˜ë¥¼ ìƒŒë‹¤. ê°¯ìˆ˜ì˜ ê²°ê³¼ê°€ [2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1] ì´ë ‡ê²Œ ë‚˜ì˜¨ë‹¤.  tip. minlengthëŠ” ì¸ë±ìŠ¤ì˜ ë²”ìœ„ë¥¼ ë‚˜íƒ€ëƒ„ ì´ ì‹ì—ì„œëŠ” 4x4 í‘œë¥¼ ë§Œë“¤ê²ƒì´ê¸° ë•Œë¬¸ì— 4**2fë¡œ ì„¤ì •\n\n\n\n\n\nconfusion_mtx.reshape(4, 4)ë¥¼ í†µí•´ ê¸¸ì´16ë²¡í„°ë¥¼ 4by4 í–‰ë ¬ë¡œ ë³€ê²½ í›„, ë°©ê¸ˆ initì—ì„œ ë§Œë“¤ì—ˆë˜ 0ìœ¼ë¡œ êµ¬ì„±ëœ 4by4 self.confusion_matrixì— ë„£ì–´ì¤€ë‹¤.\n\n\n\n\ndef print(self):\n    for i in range(self.num_classes):\n        print(f\"Class{i}:\n                {self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\nê° í´ë˜ìŠ¤ë³„ accuracyë¥¼ í‘œí˜„\n\nClass0:2.0 / 2.0  &lt;=ì •ë‹µ0 2ê°œ ì¤‘ 2ê°œë¥¼ ë§ì·„ì–´\nClass1:1.0 / 3.0  &lt;=ì •ë‹µ1 3ê°œ ì¤‘ 1ê°œë¥¼ ë§ì·„ì–´\nClass2:2.0 / 2.0  &lt;=ì •ë‹µ1 2ê°œ ì¤‘ 2ê°œë¥¼ ë§ì·„ì–´\nClass3:1.0 / 2.0  &lt;=ì •ë‹µ1 2ê°œ ì¤‘ 1ê°œë¥¼ ë§ì·„ì–´\n\n\ndef get_pix_acc(self):\n    return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\nget_pix_acc ë©”ì„œë“œëŠ” í”½ì…€ ì •í™•ë„(Pixel Accuracy)ë¥¼ ê³„ì‚°í•˜ëŠ” ì—­í•  í”½ì…€ ì •í™•ë„ëŠ” ì´ë¯¸ì§€ ë¶„í• ì´ë‚˜ ë¶„ë¥˜ ì‘ì—…ì—ì„œ ì˜ˆì¸¡ëœ í”½ì…€ì´ ì‹¤ì œ ê°’ê³¼ ì¼ì¹˜í•˜ëŠ” ë¹„ìœ¨\n\n\n\n(np.diagí–‰ë ¬ì˜ ê°€ìš´ë°ê°’ì„ ëª¨ë‘ ë”í•œê°’(6)) ë‚˜ëˆ„ê¸° (confusion_matrix.sumí–‰ë ¬ì˜ ëª¨ë“ ê°’ ë”í•œê°’(9)) get_pix_accë‹µ : 6/9\n\n\n\n\ndef get_class_acc(self):\n    class_acc =np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n    return np.nanmean(class_acc),class_acc\në„ë¡œ ì¢…ë¥˜(í´ë˜ìŠ¤)ê°€ ë¹„í¬ì¥ë„ë¡œ êµ­ë„ ê³ ì†ë„ë¡œë¡œ 3ê°œì˜ í´ë˜ìŠ¤ê°€ ìˆë‹¤ê³  í•˜ì ë„ì‹¬ì—ì„œ ë„ë¡œíƒì§€ë¥¼ í•œ ë’¤ í‰ê°€í•˜ë ¤ê³  í•˜ë©´ ë„ì‹œì— ë¹„í¬ì¥ë„ë¡œê°€ ì—†ê¸°ì— accuracyë¥¼ êµ¬í•˜ë©´ ë¹„í¬ì¥ë„ë¡œì˜ í´ë˜ìŠ¤ëŠ” 0(Nan)ê°’ì´ ë‚˜ì˜¤ê³  ì „ì²´ì  accuracyê°€ ê°ì†Œí•  ê²ƒì´ë‹¤. ì´ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œ get_class_accì„ ì“°ëŠ”ê²ƒì´ë‹¤. ë°©ë²•ì€ np.nanmean()ì„ ì“°ë©´ ëœë‹¤.\n\n\ndef get_iou(self):\n    divisor = self.confusion_matrix.sum(axis=1)\\\n                + self.confusion_matrix.sum(axis=0)\\\n                - np.diag(self.confusion_matrix)\n    iou = np.diag(self.confusion_matrix) / divisor\n    return iou\nê° í´ë˜ìŠ¤ë³„ iouë¥¼ êµ¬í•˜ëŠ” ì½”ë“œì´ë‹¤. ì¶œë ¥ê°’ì€ ë‹¤ìŒê³¼ ê°™ì´ ë‚˜ì˜¨ë‹¤. &gt;&gt;IoU: [0.66666667 0.25 0.66666667 0.5 ]\n\n\n\n\n\niou 0.39 ì˜ˆì‹œ\n\n\n\n\n\n\niouëŠ” Combined Regionë¶€ë¶„ ë‚˜ëˆ„ê¸° Overlapping ì´ë‹¤.\n\n\nIOUëŠ” ì˜ˆì¸¡í•œ í”½ì…€ê³¼ ì‹¤ì œ í”½ì…€ê°„ ê²¹ì¹¨ ì •ë„ë¥¼ í‰ê°€í•˜ëŠ” ì§€í‘œì´ë‹¤.  ì‚¬ëŒì´ ëˆˆìœ¼ë¡œ í‰ê°€í•˜ëŠ” ì§€í‘œì™€ ê°€ì¥ ìœ ì‚¬í•œ ì§€í‘œì´ë‹¤.  ì£¼ë¡œ ê°ì²´ ì¸ì‹ ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ”ë° ì‚¬ìš©ëœë‹¤.\n\n\n\n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\nget_iouì˜ í‰ê· ê°’ì´ë‹¤.  ì‚¬ì§„ì— í¬í•¨ë˜ì§€ ì•Šì€ í´ë˜ìŠ¤ê°€ ìˆì„ ê²½ìš° iouê°’ì´ Nanê°’ìœ¼ë¡œ ë‚˜ì˜¤ê²Œ ëœë‹¤.  Nanê°’ì„ ì œê±°í•˜ê³  ì •í™•í•œ í‰ê·  ê³„ì‚°ì„ ìœ„í•´ np.nansum()ìœ¼ë¡œ ë”í•´ì¤€ë‹¤.\n\n\n\n\n\nex_targets=torch.Tensor([[[0,1,2],[3,2,1],[0,3,1]]])#shape=(1,3,3)=(batchsize,H,W)\nex_preds=torch.Tensor([[[[1,0,0], \n                     [0,0,0],\n                     [1,0,1]],\n                    [[0,0,0],\n                     [1,0,1],\n                     [0,0,0]],\n                    [[0,1,1],\n                     [0,1,0],\n                     [0,0,0]],\n                    [[0,0,0],\n                     [0,0,0],\n                     [0,1,0]]]]) #shape=(1,4,3,3)=(batchsize,C,H,W)\n\n\n# ConfusionMatrix í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤í™”\nnum_classes = 4  # 0 ~ 10 ì‚¬ì´ì˜ ìˆ«ìë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ 11ê°œì˜ í´ë˜ìŠ¤\nconfusion_matrix = ConfusionMatrix(num_classes)\n\n# ë°°ì¹˜ ì²˜ë¦¬\nconfusion_matrix.process_batch(ex_preds, ex_targets)\n\n# ë©”íŠ¸ë¦­ ì¶œë ¥\nconfusion_matrix.print()\nprint(\"process_batch:\", confusion_matrix.process_batch(ex_preds, ex_targets))\nprint(\"Pixel Accuracy:\", confusion_matrix.get_pix_acc())\nprint(\"Class Accuracy:\", confusion_matrix.get_class_acc())\nprint(\"IoU:\", confusion_matrix.get_iou())\nprint(\"Mean IoU:\", confusion_matrix.get_mean_iou())\n\n\nê²°ê³¼\nClass0:2.0 / 2.0  &lt;=ì •ë‹µ0 2ê°œ ì¤‘ 2ê°œë¥¼ ë§ì·„ì–´\nClass1:1.0 / 3.0  &lt;=ì •ë‹µ1 3ê°œ ì¤‘ 1ê°œë¥¼ ë§ì·„ì–´\nClass2:2.0 / 2.0  &lt;=ì •ë‹µ1 2ê°œ ì¤‘ 2ê°œë¥¼ ë§ì·„ì–´\nClass3:1.0 / 2.0  &lt;=ì •ë‹µ1 2ê°œ ì¤‘ 1ê°œë¥¼ ë§ì·„ì–´\nprocess_batch:\n[[2 0 0 0]\n[1 1 1 0]\n[0 0 2 0]\n[0 1 0 1]]\nPixel Accuracy: 0.6666\nClass Accuracy: 0.7083\nIoU: [0.66666667 0.25 0.66666667 0.5 ]\nMean IoU: 0.52083\n\n\n\n\n\n\n\n\n\n\nLoss\n\ndef bce_loss(preds, targets, pos_weight=None):\n    bce_loss = F.binary_cross_entropy_with_logits(\n        preds.float(),\n        targets.float(),\n        pos_weight=pos_weight,\n    )\n    return bce_loss\n\n\ndef ce_loss(preds, targets, ignore=255):\n    ce_loss = F.cross_entropy(\n        preds.float(),\n        targets.long(),    # [B, H, W]\n        ignore_index=ignore,\n    )\n    return ce_loss\n\n\ndef dice_loss(preds, targets, eps=1e-7):\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)   # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)                        # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))        # dims = (0, 2, 3)\n    intersection = torch.sum(probas * true_1_hot, dims)     # intersection w.r.t. the class\n    cardinality = torch.sum(probas + true_1_hot, dims)      # cardinality w.r.t. the class\n    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n    return (1 - dice_loss)\n\n\ndef jaccard_loss(preds, targets, eps=1e-7):\n    \"\"\"Computes the Jaccard loss.\n    Args:\n    preds(logits) a tensor of shape [B, C, H, W]\n    targets: a tensor of shape [B, 1, H, W].\n    eps: added to the denominator for numerical stability.\n    Returns:\n        Jaccard loss\n    \"\"\"\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return (1 - jacc_loss)\n\n\n bce_loss ce_lossdice_lossjaccard_loss\n\n\nê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ê³„ì‚°\nArgs:\n    targets: [B, 1, H, W] í˜•íƒœì˜ í…ì„œ. ì‹¤ì œ ê°’ (ë ˆì´ë¸”).\n    preds: [B, 1, H, W] í˜•íƒœì˜ í…ì„œ. ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’.\n    pos_weight: ì–‘ì„± í´ë˜ìŠ¤ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ (ì„ íƒ ì‚¬í•­).\n\nReturns:\n    bce_loss: ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ì´ì§„ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤.\n\n\n\nBCE LOSS\n\n\n\nN: ë°ì´í„°ì˜ ìˆ˜\nyğ‘–ğ‘ : ì‹¤ì œ ë ˆì´ë¸” (0 ë˜ëŠ” 1)\npğ‘–ğ‘ : ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í´ë˜ìŠ¤ 1ì— ì†í•  í™•ë¥ \n\nëª©ì : BCEì˜ ì£¼ëœ ëª©ì ì€ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ğ‘ğ‘–ê°€ ì‹¤ì œ ë ˆì´ë¸” ğ‘¦ğ‘–ì™€ ì–¼ë§ˆë‚˜ ì˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë ˆì´ë¸” ê°„ì˜ ì°¨ì´ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤.\nì†ì‹¤ ê³„ì‚°: BCEëŠ” ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ ì˜ˆì¸¡ê°’ ğ‘ğ‘–ê°€ ì‹¤ì œ ë ˆì´ë¸” ğ‘¦ğ‘–ì™€ ì–¼ë§ˆë‚˜ ë¹„ìŠ·í•œì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ì˜ˆì¸¡ê°’ì´ ì‹¤ì œ ë ˆì´ë¸”ê³¼ ì •í™•íˆ ì¼ì¹˜í•  ë•Œ ì†ì‹¤ì€ 0ì´ ë˜ê³ , ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì†ì‹¤ì´ ì¦ê°€í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ë„ë¡ ìœ ë„í•©ë‹ˆë‹¤.\ní™•ë¥ ì  í•´ì„: BCEëŠ” ê° ì˜ˆì¸¡ê°’ ğ‘ğ‘–ì„ í´ë˜ìŠ¤ 1ì— ì†í•  í™•ë¥ ë¡œ í•´ì„í•˜ë©°, ì´ë¥¼ ë¡œê·¸ í•¨ìˆ˜ë¥¼ í†µí•´ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ í´ë˜ìŠ¤ 1ì— ì†í•  í™•ë¥ ì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.\nìµœì í™”: BCEëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì´ì§„ ë¶„ë¥˜ ëª¨ë¸ì˜ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‚¬ìš©ë˜ë©°, ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë¸ì´ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ê³ , ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì •í™•í•œ ì˜ˆì¸¡ì„ í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n\n\nê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ë‹¤ì¤‘ í´ë˜ìŠ¤ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤ì„ ê³„ì‚°\n\nArgs:\n    targets: [B, H, W] í˜•íƒœì˜ í…ì„œ. ì‹¤ì œ ê°’ (ë ˆì´ë¸”).\n    preds: [B, C, H, W] í˜•íƒœì˜ í…ì„œ. ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’.\n    ignore: ë¬´ì‹œí•  í´ë˜ìŠ¤ ì¸ë±ìŠ¤.\n\nReturns:\n    ce_loss: ê°€ì¤‘ì¹˜ê°€ ì ìš©ëœ ë‹¤ì¤‘ í´ë˜ìŠ¤ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì†ì‹¤.\n\n\n\nCE LOSS\n\n\n\nN: ë°ì´í„°ì˜ ìˆ˜\nC: í´ë˜ìŠ¤ì˜ ìˆ˜\nyğ‘–ğ‘ : ë°ì´í„° í¬ì¸íŠ¸ iì˜ ì‹¤ì œ í´ë˜ìŠ¤ ğ‘ì— ëŒ€í•œ one-hot ì¸ì½”ë”©ëœ ë ˆì´ë¸” (0 ë˜ëŠ” 1)\npğ‘–ğ‘ : ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë°ì´í„° í¬ì¸íŠ¸ ğ‘–ê°€ í´ë˜ìŠ¤ cì— ì†í•  í™•ë¥ \n\nëª©ì : CE ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ëª¨ë¸ì—ì„œ ì˜ˆì¸¡ê°’ picê°€ ì‹¤ì œ ë ˆì´ë¸”yicì™€ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ëª¨ë¸ì´ ë‹¤ì–‘í•œ í´ë˜ìŠ¤ë¥¼ ë¶„ë¥˜í•˜ê³ , ê° í´ë˜ìŠ¤ì— ì†í•  í™•ë¥ ì„ ì •í™•í•˜ê²Œ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµì‹œí‚¤ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\nì†ì‹¤ ê³„ì‚°: ê° ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ CEëŠ” ê° í´ë˜ìŠ¤ì— ëŒ€í•´ ì˜ˆì¸¡ëœ í™•ë¥  picê°€ ì‹¤ì œ ë ˆì´ë¸” yic ì‚¬ì´ì˜ êµì°¨ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•˜ì—¬ ì „ì²´ ë°ì´í„°ì…‹ì— ëŒ€í•œ í‰ê·  ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ ì†ì‹¤ì€ ëª¨ë¸ì´ ì˜ˆì¸¡ì„ í–¥ìƒì‹œí‚¤ê³ , ë‹¤ì–‘í•œ í´ë˜ìŠ¤ ê°„ì˜ ê²°ì • ê²½ê³„ë¥¼ ëª…í™•í•˜ê²Œ ë§Œë“¤ë„ë¡ ë•ìŠµë‹ˆë‹¤.\ní™•ë¥ ì  í•´ì„: CE ì†ì‹¤ í•¨ìˆ˜ëŠ” ì˜ˆì¸¡ëœ í™•ë¥  picì„ í´ë˜ìŠ¤ cì— ì†í•  í™•ë¥ ë¡œ í•´ì„í•˜ë©°, ì´ë¥¼ ë¡œê·¸ í•¨ìˆ˜ë¥¼ í†µí•´ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ëª¨ë¸ì€ ê° ë°ì´í„° í¬ì¸íŠ¸ê°€ ê° í´ë˜ìŠ¤ì— ì†í•  ê°€ëŠ¥ì„±ì„ ì •í™•íˆ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤.\nìµœì í™”: CE ì†ì‹¤ í•¨ìˆ˜ëŠ” ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œì—ì„œ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¤ëŠ” ë° ì£¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ê²½ì‚¬ í•˜ê°•ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì •í•˜ì—¬ CE ì†ì‹¤ì„ ìµœì†Œí™”í•˜ë„ë¡ í•˜ì—¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.\n\n\nSÃ¸rensenâ€“Dice ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n\nArgs:\n    preds(logits): [B, C, H, W] í˜•íƒœì˜ í…ì„œ. ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ (ë¡œì§“).\n    targets: [B, 1, H, W] í˜•íƒœì˜ í…ì„œ. ì‹¤ì œ ê°’ (ë ˆì´ë¸”).\n    eps: ë¶„ëª¨ì— ë”í•´ì§€ëŠ” ì‘ì€ ê°’ìœ¼ë¡œ, ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\nReturns:\n    dice_loss: SÃ¸rensenâ€“Dice ì†ì‹¤ ê°’.\n\n\n\nDICE LOSS\n\n\n\nâˆ£Pâˆ©Tâˆ£: ì˜ˆì¸¡ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ì—­ Pê³¼ ì‹¤ì œ íƒ€ê²Ÿ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ì—­ Tì˜ êµì§‘í•©ì˜ í¬ê¸°ì…ë‹ˆë‹¤.\nâˆ£Pâˆ£: ì˜ˆì¸¡ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ì—­ Pì˜ í¬ê¸° ë˜ëŠ” ì›ì†Œ ìˆ˜ì…ë‹ˆë‹¤.\nâˆ£Tâˆ£: ì‹¤ì œ íƒ€ê²Ÿ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ì—­ Tì˜ í¬ê¸° ë˜ëŠ” ì›ì†Œ ìˆ˜ì…ë‹ˆë‹¤.\n\nëª©ì : Dice ê³„ìˆ˜ëŠ” ë‘ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ì—­ì˜ ì¤‘ì²© ì •ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ì˜ˆì¸¡ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ ğ‘ƒì´ ì‹¤ì œ íƒ€ê²Ÿ ì„¸ê·¸ë©˜í…Œì´ì…˜ Tê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì´ ì§€í‘œëŠ” 0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‘ ì˜ì—­ì´ ì™„ì „íˆ ì¼ì¹˜í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nê³„ì‚° ë°©ë²•: Dice ê³„ìˆ˜ì˜ ë¶„ì 2Ã—âˆ£ğ‘ƒâˆ©ğ‘‡âˆ£ëŠ” ì˜ˆì¸¡ëœ ì˜ì—­ê³¼ ì‹¤ì œ íƒ€ê²Ÿ ì˜ì—­ì˜ êµì§‘í•©ì˜ í¬ê¸°ë¥¼ ë‘ ë°°ë¡œ í™•ì¥í•˜ì—¬, under-segmentationê³¼ over-segmentationì„ ëª¨ë‘ ê³µí‰í•˜ê²Œ íŒ¨ë„í‹°ë¥¼ ì¤ë‹ˆë‹¤. ë¶„ëª¨ âˆ£Pâˆ£+âˆ£Tâˆ£ëŠ” ë‘ ì˜ì—­ì˜ í¬ê¸° í•©ìœ¼ë¡œ ì •ê·œí™”í•˜ì—¬ ê³„ìˆ˜ê°€ [0, 1] ë²”ìœ„ ë‚´ì— ìœ ì§€ë˜ë„ë¡ í•©ë‹ˆë‹¤.\nì‘ìš©: Dice ê³„ìˆ˜ëŠ” ì£¼ë¡œ ì˜ë£Œ ì´ë¯¸ì§€ ë¶„ì„ ë° ë‹¤ì–‘í•œ ì˜ìƒ ì²˜ë¦¬ ì• í”Œë¦¬ì¼€ì´ì…˜ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì„¸ë¶„í™” ì•Œê³ ë¦¬ì¦˜ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ê³ , ì˜ˆì¸¡ëœ ì„¸ê·¸ë©˜í…Œì´ì…˜ì˜ ì§ˆì„ ë¹„êµí•˜ëŠ” ë° ì¤‘ìš”í•œ ì§€í‘œ ì—­í• ì„ í•©ë‹ˆë‹¤.\n\n\n    Jaccard ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n    ì¸ìˆ˜:\n    preds(logits): í˜•ìƒì´ [B, C, H, W]ì¸ í…ì„œ\n    targets: í˜•ìƒì´ [B, 1, H, W]ì¸ í…ì„œ\n    eps: ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•´ ë¶„ëª¨ì— ì¶”ê°€ë˜ëŠ” ì‘ì€ ê°’\n    ë°˜í™˜:\n    Jaccard ì†ì‹¤\n\n\n\nJACCARD LOSS\n\n\n\nâˆ£Pâˆ©Tâˆ£: ì˜ˆì¸¡ëœ ì˜ì—­ ğ‘ƒê³¼ ì‹¤ì œ íƒ€ê²Ÿ ì˜ì—­ ğ‘‡ì˜ êµì§‘í•©ì˜ í¬ê¸°\nâˆ£PâˆªTâˆ£: ì˜ˆì¸¡ëœ ì˜ì—­ ğ‘ƒê³¼ ì‹¤ì œ íƒ€ê²Ÿ ì˜ì—­ ğ‘‡ì˜ í•©ì§‘í•©ì˜ í¬ê¸°\n\nëª©ì : Jaccard ì§€ìˆ˜ëŠ” ë‘ ì„¸ê·¸ë©˜í…Œì´ì…˜ ì˜ì—­ì˜ ì¤‘ì²© ì •ë„ë¥¼ ì¸¡ì •í•˜ì—¬ ì˜ˆì¸¡ëœ ì„¸ê·¸ë©˜í…Œì´ Pì´ ì‹¤ì œ íƒ€ê²Ÿ ì„¸ê·¸ë©˜í…Œì´ì…˜ Tê³¼ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€ë¥¼ í‰ê°€í•©ë‹ˆë‹¤. ì´ ì§€í‘œëŠ” 0ì—ì„œ 1 ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë‘ ì˜ì—­ì´ ì™„ì „íˆ ì¼ì¹˜í•¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\nê³„ì‚° ë°©ë²•: Jaccard ì§€ìˆ˜ëŠ” ì˜ˆì¸¡ëœ ì˜ì—­ ğ‘ƒê³¼ ì‹¤ì œ íƒ€ê²Ÿ ì˜ì—­ ğ‘‡ì˜ êµì§‘í•© í¬ê¸°ë¥¼ ë‘ ì˜ì—­ì˜ í•©ì§‘í•© í¬ê¸°ë¡œ ë‚˜ëˆ„ì–´ ê³„ì‚°í•©ë‹ˆë‹¤. ì´ëŠ” ë‘ ì˜ì—­ì´ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ë¥¼ í‰ê°€í•˜ë©°, ë¶„ìëŠ” êµì§‘í•©ì˜ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ê³  ë¶„ëª¨ëŠ” ì „ì²´ì ì¸ ì˜ˆì¸¡ëœ ì˜ì—­ê³¼ ì‹¤ì œ íƒ€ê²Ÿ ì˜ì—­ì˜ í¬ê¸°ë¥¼ ë°˜ì˜í•©ë‹ˆë‹¤.\nì‘ìš©: Jaccard ì§€ìˆ˜ëŠ” ì£¼ë¡œ ì´ë¯¸ì§€ ì„¸ë¶„í™”ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ì´ë¯¸ì§€ ì²˜ë¦¬ ë° íŒ¨í„´ ì¸ì‹ ë¬¸ì œì—ì„œ ìœ ì‚¬ì„±ì„ ë¹„êµí•˜ê³  ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ í‰ê°€í•˜ëŠ” ë° ì¤‘ìš”í•œ ì§€í‘œë¡œ í™œìš©ë©ë‹ˆë‹¤.\n\n\n\n\n\none_epoch( train, val )\n\ndef train_one_epoch(train_dataloader, model, optimizer, device):\n    model.train()\n    losses = [] \n    for i, (imgs, targets, _) in enumerate(train_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        preds = model(imgs)['out']     # forward \n        loss = ce_loss(preds, targets) # calculates the iteration loss  \n        optimizer.zero_grad()   # zeros the parameter gradients\n        loss.backward()         # backward\n        optimizer.step()        # update weights\n        print('\\t iteration: %d/%d, loss=%.4f' % (i, len(train_dataloader)-1, loss))    \n        losses.append(loss.item())\n    return torch.tensor(losses).mean().item()\n\n\ndef val_one_epoch(val_dataloader, model, confusion_matrix, device):\n    model.eval()\n    losses = []\n    for i, (imgs, targets, img_file) in enumerate(val_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        with torch.no_grad():\n            preds = model(imgs)['out']   # forward, preds: (B, 2, H, W)\n            loss = ce_loss(preds, targets)\n            losses.append(loss.item())\n            confusion_matrix.process_batch(preds, targets)\n            # sample images\n            if i == 0:\n                preds = torch.argmax(preds, axis=1) # (1, H, W)  \n                for j in range(3):\n                    save_file = os.path.join('outputs', 'val_%d.png' % (j))\n                    plot_image(imgs[j], preds[j], save_file)\n                \n    avg_loss = torch.tensor(losses).mean().item()\n    \n    return avg_loss\n\n\ntrain_one_epochval_one_epoch\n\n\n1) def train_one_epoch(train_dataloader, model, optimizer, device)ì´ë¼ëŠ”\n    í•¨ìˆ˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ 4ê°œì´ë‹¤.\n2) model.train()ì€ í•™ìŠµëª¨ë“œì¸ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.\n3) losses = []ëŠ” trainì˜ lossê°’ì„ ì €ì¥í•˜ê¸° ìœ„í•¨ì´ë‹¤.\n4) ë°˜ë³µë¬¸(for)ì˜ i ëŠ” enumerateí•¨ìˆ˜ì—ì„œ ë§Œë“¤ì–´ì§„(ì¸ë±ì‹±ëœ) 0ë¶€í„° nê¹Œì§€ì˜\n    ìˆ«ìì´ê³  (imgs, targets, _)ëŠ” ê°ê° imgsëŠ” ìœ„ì„±ì‚¬ì§„ targetsëŠ” ì •ë‹µë¼ë²¨ì´ë‹¤.\n5) deviceë¥¼ í†µí•´ cpuì—ì„œ gpuë¡œ ì´ë™ì‹œí‚¤ëŠ” ê³¼ì •ìœ¼ë¡œ deviceëŠ” ë’¤ì—ì„œ ì •ì˜í•  ê²ƒì´ë‹¤.\n6) modelì— imgë¥¼ ì§‘ì–´ë„£ì–´ predsë¥¼ ì˜ˆì¸¡í•´ë³¸ë‹¤. ì´ë•Œ [out]ì´ ë¶™ëŠ” ì´ìœ ëŠ”\n    torchvisionì— ì €ì¥ë˜ì–´ ìˆëŠ” deeplabv3ëª¨ë¸ì„ ì‚¬ìš©í• ë•Œì˜ ë°©ì‹ì´ë‹¤.\n7) lossëŠ” ì•ì—ì„œ ì •ì˜í•´ì¤€ ë¡œìŠ¤ ì¤‘ ce_lossë¥¼ ì‚¬ìš©í–ˆë‹¤. \n    ì´ë•Œ predsê°€ ë¨¼ì € ë“¤ì–´ê°€ì•¼í•˜ëŠ” ì ì„ ì£¼ì˜í•˜ì\n8) optimizer.zeroëŠ” lossì˜ ë¯¸ë¶„ê°’ì„ ê³„ì‚°í•˜ê¸°ì „ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ëŠ” ê³¼ì •ì´ë‹¤.\n9) loss.backward()ëŠ” lossê°’ì„ í¸ë¯¸ë¶„í•˜ëŠ” ê³¼ì •ì´ë‹¤. \n    í¸ë¯¸ë¶„ì€ 2ì°¨ì› ê³µê°„ì—ì„œ ë¯¸ë¶„ì„ í•œ ë²ˆì— ëª»í•˜ë‹ˆê¹ \n    ìˆœì°¨ì ìœ¼ë¡œ ë¯¸ë¶„í•˜ëŠ” ê³¼ì •ì´ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤.\n10) ë¯¸ë¶„í•œ ê°’ì„ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°’ì— ë°˜ì˜í•œë‹¤(ë¯¸ë¶„ê°’ì„ ë¹¼ì£¼ëŠ”ê³¼ì •). \n11) iterationë³„ train lossë¥¼ í‘œì‹œí•´ì¤€ë‹¤.\n12) lossë¥¼ lossesì— ì €ì¥í•œë‹¤.\n13) returnì„ í†µí•´ í•¨ìˆ˜ ë°–ì—ì„œë„ lossesì˜ í‰ê· ì„ í˜¸ì¶œ í•  ìˆ˜ ìˆê²Œí•œë‹¤.\n\n\n1) def val_one_epoch(val_dataloader, model, confusion_matrix, device)ì´ë¼ëŠ”\n    í•¨ìˆ˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ 4ê°œì´ë‹¤.\n2) model.eval()ì€ í•™ìŠµëª¨ë“œì¸ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.\n3) losses = []ëŠ” validationì˜ lossê°’ì„ ì €ì¥í•˜ê¸° ìœ„í•¨ì´ë‹¤.\n4) ë°˜ë³µë¬¸(for)ì˜ i ëŠ” enumerateí•¨ìˆ˜ì—ì„œ ë§Œë“¤ì–´ì§„(ì¸ë±ì‹±ëœ) 0ë¶€í„° nê¹Œì§€ì˜\n    ìˆ«ìì´ê³  (imgs, targets, _)ëŠ” ê°ê° imgsëŠ” ìœ„ì„±ì‚¬ì§„ targetsëŠ” ì •ë‹µë¼ë²¨ì´ë‹¤.\n5) deviceë¥¼ í†µí•´ cpuì—ì„œ gpuë¡œ ì´ë™ì‹œí‚¤ëŠ” ê³¼ì •ìœ¼ë¡œ deviceëŠ” ë’¤ì—ì„œ ì •ì˜í•  ê²ƒì´ë‹¤.\n6) modelì— imgë¥¼ ì§‘ì–´ë„£ì–´ predsë¥¼ ì˜ˆì¸¡í•´ë³¸ë‹¤. ì´ë•Œ [out]ì´ ë¶™ëŠ” ì´ìœ ëŠ”\n    torchvisionì— ì €ì¥ë˜ì–´ ìˆëŠ” deeplabv3ëª¨ë¸ì„ ì‚¬ìš©í• ë•Œì˜ ë°©ì‹ì´ë‹¤.\n7) lossëŠ” ì•ì—ì„œ ì •ì˜í•´ì¤€ ë¡œìŠ¤ ì¤‘ ce_lossë¥¼ ì‚¬ìš©í–ˆë‹¤. \n    ì´ë•Œ predsê°€ ë¨¼ì € ë“¤ì–´ê°€ì•¼í•˜ëŠ” ì ì„ ì£¼ì˜í•˜ì\n8) lossë¥¼ lossesì— ì €ì¥í•œë‹¤.\n9) ì´í›„ì— ConfusionMatrixë¥¼ ì´ìš©í•´ confusion_matrixë¼ëŠ” ì¸ìŠ¤í„´ìŠ¤ë¥¼ ë§Œë“¤ê²Œ\n ë˜ëŠ”ë° ë§Œë“¤ì–´ì§„ ì¸ìŠ¤í„´ìŠ¤ì— ëª¨ë¸ì´ ì˜ˆì¸¡í•œ predsì™€ ì •ë‹µì§€ targetsì„ ë„£ì–´ì£¼ë©´ \n í–‰ë ¬ì´ ê³„ì‚°ì´ ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ì‚¬ìš©ê°€ëŠ¥í•˜ë‹¤.\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc() \n\n12) ~ 16) i=0ì¼ë•Œ(epochì´ ìƒˆë¡œ ì‹œì‘ë ë•Œ)ë§ˆë‹¤ plot_image í•¨ìˆ˜ë¥¼ ì´ìš©í•´ \nimgì— predsë¥¼ ì˜¤ë²„ë ˆì´í•´ì„œ ì‚¬ì§„ìœ¼ë¡œ ì €ì¥í•œë‹¤.(ì´ 3ì¥ ì €ì¥í•œë‹¤.) \nì¶”ê°€ë¡œ ì €ì¥ëœ ì‚¬ì§„ì€ ì´ë¦„ì´ val_0(1,2).pngìœ¼ë¡œ ë§¤ ì—í­ë§ˆë‹¤ ì‚¬ì§„ì´ ë°”ë€ë‹¤. \në”°ë¡œ ì €ì¥í•˜ë ¤ë©´ ìˆ˜ì •í•„ìš”\n17~) returnì„ í†µí•´ í•¨ìˆ˜ ë°–ì—ì„œë„ validation lossesì˜ í‰ê· ì„ í˜¸ì¶œ í•  ìˆ˜ ìˆê²Œí•œë‹¤.\n\n\n\n\n\nTrain\n\n#def train______________________________________________________#   \ndef train(epochs=200, batch_size = 8 , name = 'suwany'):\n#1______________________________________________________________#    \n    #Â wandbÂ settings\n    wandb.init(id=name, resume='allow', mode='disabled')\n    wandb.config.update({\n        'epochs': epochs,\n        'batch_size': batch_size,\n        'name': name\n    })\n#2______________________________________________________________#   \n    # Train dataset\n    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n    # Train dataloader\n    num_workers = min([os.cpu_count(), batch_size, 16])\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    # Validation dataset\n    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n#3______________________________________________________________#   \n    # Network model\n    num_classes = 10 # background + 1 classes\n    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n    \n    # GPU-support\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.device_count() &gt; 1:   # multi-GPU\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n#______________________________________________________________#   \n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n      \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n#______________________________________________________________#   \n    # loading a weight file (if exists)\n    weight_file = Path('weights')/(name + '.pth')\n    best_accuracy = 0.0\n    start_epoch, end_epoch = (0, epochs)\n    if os.path.exists(weight_file):\n        checkpoint = torch.load(weight_file)\n        model.load_state_dict(checkpoint['model'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_accuracy = checkpoint['best_accuracy']\n        print('resumed from epoch %d' % start_epoch)\n#______________________________________________________________#       \n    \n    confusion_matrix = ConfusionMatrix(num_classes)\n # training/validation\n    for epoch in range(start_epoch, end_epoch):\n        print('epoch: %d/%d' % (epoch, end_epoch-1))\n        t0 = time.time()\n        # training\n        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n        t1 = time.time()\n        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n        lr_scheduler.step(epoch_loss)\n        # validation\n        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n        \n        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n        # saving the best status into a weight file\n        if val_epoch_pix_accuracy &gt; best_accuracy:\n             best_weight_file = Path('weights')/(name + '_best.pth')\n             best_accuracy = val_epoch_pix_accuracy\n             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n             torch.save(state, best_weight_file)\n             print('best accuracy=&gt;saved\\n')\n        # saving the current status into a weight file\n        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n        torch.save(state, weight_file)\n        # wandb logging\n        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n#______________________________________________________________#          \n\n\ní•¨ìˆ˜ train ì†Œê°œdef trainwandbdataset, dataloadernum_classes, model, device Optimizer,lr_scheduler loadtrain_start\n\n\nì´ í•¨ìˆ˜ëŠ” ì§€ê¸ˆê¹Œì§€ ì •ì˜í•œ í•¨ìˆ˜ë“¤ì„ ì´ìš©í•¨, ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê²€ì¦í•˜ëŠ” ê³¼ì •ì„ í¬í•¨ë˜ì–´ìˆë‹¤. ë˜í•œ, ëª¨ë¸ì˜ ìƒíƒœë¥¼ ì €ì¥í•˜ê³ , WandB(Weights and Biases)ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¡œê·¸ë¥¼ ê¸°ë¡í•œë‹¤.\n\n\ndef train(epochs=200, batch_size = 8 , name = 'suwany'):\n\n\nepochs: í•™ìŠµí•  ì´ ì—í¬í¬ ìˆ˜ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ 200ì…ë‹ˆë‹¤.\nbatch_size: ë¯¸ë‹ˆë°°ì¹˜ì˜ í¬ê¸°ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ 8ì…ë‹ˆë‹¤.\nname: ì‹¤í—˜ ì´ë¦„ì…ë‹ˆë‹¤. ê¸°ë³¸ê°’ì€ â€™suwanyâ€™ì…ë‹ˆë‹¤.\n\n\n\n    wandb.init(id=name, resume='allow', mode='disabled')\n    wandb.config.update({\n        'epochs': epochs,\n        'batch_size': batch_size,\n        'name': name\n    })\n\nWandBë¥¼ ì´ˆê¸°í™”í•˜ê³  ì„¤ì •ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì´ë¡œ ì¸í•´ ì‹¤í—˜ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ê°€ ê¸°ë¡ë©ë‹ˆë‹¤\n\n\n\n    # Train dataset\n    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n    # Train dataloader\n    num_workers = min([os.cpu_count(), batch_size, 16])\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    # Validation dataset\n    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n\nKariRoadDataset í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.\nê° ë°ì´í„°ì…‹ì— ëŒ€í•´ ë°ì´í„°ë¡œë”ë¥¼ ìƒì„±í•˜ì—¬ ë°ì´í„°ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë¡œë“œí•©ë‹ˆë‹¤.\nnum_workersëŠ” ë°ì´í„° ë¡œë”©ì„ ìœ„í•œ CPU ìŠ¤ë ˆë“œ ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.\n\n\n\n    num_classes = 10  # background + 9 classes\n    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n    \n    # GPU-support\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.device_count() &gt; 1:   # multi-GPU\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n\nnum_classesë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë¸ì˜ í´ë˜ìŠ¤ ìˆ˜ë¥¼ ì •ì˜í•œë‹¤.\n\nì—¬ê¸°ì„œëŠ” roadì˜ í´ë˜ìŠ¤ 9ê°œì— ë°°ê²½ 1ê°œë¥¼ ì¶”ê°€í•œë‹¤.\n\nDeepLabV3 ëª¨ë¸ì„ ResNet-101 ë°±ë³¸ì„ ì‚¬ìš©í•˜ì—¬ ì´ˆê¸°í™”í–ˆë‹¤.\nëª¨ë¸ì„ GPUë¡œ ì´ë™ì‹œí‚¤ë©°, ì—¬ëŸ¬ GPUë¥¼ ì‚¬ìš©í•  ê²½ìš° DataParallelì„ ì‚¬ìš©í•˜ë„ë¡ í–ˆë‹¤.\n\n\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n      \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n\nAdam ì˜µí‹°ë§ˆì´ì €ë¥¼ ì„¤ì •í•˜ì—¬ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™”í•œë‹¤.\ní•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ê²€ì¦ ì†ì‹¤ì´ 5íšŒ ì´ìƒ ì¤„ì–´ë“¤ì§€ ì•Šì„ ë•Œ í•™ìŠµë¥ ì„ ê°ì†Œì‹œí‚¤ë„ë¡ ì„¤ì •í–ˆë‹¤.\n\n\n\n    weight_file = Path('weights')/(name + '.pth')\n    best_accuracy = 0.0\n    start_epoch, end_epoch = (0, epochs)\n    if os.path.exists(weight_file):\n        checkpoint = torch.load(weight_file)\n        model.load_state_dict(checkpoint['model'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_accuracy = checkpoint['best_accuracy']\n        print('resumed from epoch %d' % start_epoch)\n\nê¸°ì¡´ì— ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì´ ìˆìœ¼ë©´ ì´ë¥¼ ë¡œë“œí•˜ì—¬ í•™ìŠµì„ ì´ì–´ì„œ ì§„í–‰í•  ìˆ˜ ìˆë‹¤.\n\n\n    confusion_matrix = ConfusionMatrix(num_classes)\n    # training/validation\n    for epoch in range(start_epoch, end_epoch):\n        print('epoch: %d/%d' % (epoch, end_epoch-1))\n        t0 = time.time()\n        # training\n        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n        t1 = time.time()\n        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n        lr_scheduler.step(epoch_loss)\n        # validation\n        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n        \n        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n        # saving the best status into a weight file\n        if val_epoch_pix_accuracy &gt; best_accuracy:\n             best_weight_file = Path('weights')/(name + '_best.pth')\n             best_accuracy = val_epoch_pix_accuracy\n             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n             torch.save(state, best_weight_file)\n             print('best accuracy=&gt;saved\\n')\n        # saving the current status into a weight file\n        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n        torch.save(state, weight_file)\n        # wandb logging\n        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n\n\nforë¬¸ì„ ì´ìš©í•´ ê° ì—í¬í¬ë§ˆë‹¤ í•™ìŠµ ë° ê²€ì¦ì„ ìˆ˜í–‰í•œë‹¤.\ntrain_one_epoch í•¨ìˆ˜ëŠ” í•œ ì—í¬í¬ ë™ì•ˆ í•™ìŠµì„ ìˆ˜í–‰í•˜ë©°, ì†ì‹¤ ê°’ì„ ë°˜í™˜í–ˆë‹¤.\nê²€ì¦ ë‹¨ê³„ì—ì„œëŠ” val_one_epoch í•¨ìˆ˜ë¥¼ í†µí•´ ê²€ì¦ ì†ì‹¤, IoU, í‰ê·  IoU, í”½ì…€ ì •í™•ë„ë¥¼ ê³„ì‚°í•œë‹¤.\nìµœìƒì˜ ëª¨ë¸ ìƒíƒœë¥¼ ì €ì¥í•˜ë©°, í˜„ì¬ ìƒíƒœë„ ì—í­ë§ˆë‹¤ ì €ì¥ë˜ë„ë¡ í–ˆë‹¤.\nWandBë¥¼ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ ë° ê²€ì¦ ì†ì‹¤, ì •í™•ë„ë¥¼ ë¡œê¹…ë˜ì–´ ê¸°ë¡ëœë‹¤.\n\n\n\n\n\n\nì‹¤í–‰ ì½”ë“œ\n\ntrain(epochs=200, batch_size = 8 , name = 'suwany')\n\nepoch: 0/199\n     iteration: 0/142, loss=2.3710\n     iteration: 1/142, loss=2.3142\n     iteration: 2/142, loss=2.2351\n     iteration: 3/142, loss=2.2264\n     iteration: 4/142, loss=2.1433\n     iteration: 5/142, loss=2.0343\n     iteration: 6/142, loss=1.9032",
    "crumbs": [
      "About",
      "Posts",
      "Post with Code",
      "Road_segmantation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "semantic-segmentation",
    "section": "",
    "text": "ê°œìš”\nImage semantic-segmentation using kari-dataset The goal of this project is to label every pixel in an image, dividing it into meaningful segments.\n\nThis work also references code from Dr.Â Ohhan.\nëª©ì°¨\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 25, 2024\n\n\nRoad_segmantation\n\n\nÂ \n\n\n\n\n\nNo matching items"
  }
]