[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/post-with-code/road_seg.html",
    "href": "posts/post-with-code/road_seg.html",
    "title": "Road_segmantation",
    "section": "",
    "text": "This is a post with executable code.\n\nImports\n\n#Datasets\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n#Utils\nimport os\nimport cv2\nimport numpy as np\nimport torch\n\n#ConfusionMatrix\nimport numpy as np\n\n#Loss\nimport torch\nimport torch.nn.functional as F\n\n#Train\nimport os\nimport argparse\nimport torch\nimport torch.optim as optim\nimport time\nimport wandb\nfrom pathlib import Path\nfrom torchvision import models\n\n\n\nDataset\n\nclass KariRoadDataset(torch.utils.data.Dataset):\n    def __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\n\n    def __getitem__(self, idx):\n        img_file= self.img_files[idx].as_posix()\n        label_file = img_file.replace('images', 'labels')\n        img = cv2.imread(img_file)\n        label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n        img, label = self.transform(img, label)\n        return img, label, img_file\n\n    def __len__(self):\n        return len(self.img_files)\n    \nclass ImageAug:\n    def __init__(self, train):\n        if train:\n            self.aug = A.Compose([A.RandomCrop(256, 256),\n                                  A.HorizontalFlip(p=0.5),\n                                  A.ShiftScaleRotate(p=0.3),\n                                  A.RandomBrightnessContrast(p=0.3),\n                                  A.pytorch.transforms.ToTensorV2()])\n        else:\n            self.aug = ToTensorV2()\n\n    def __call__(self, img, label):\n        transformed = self.aug(image=img, mask=np.squeeze(label))\n        return transformed['image']/255.0, transformed['mask']\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\nKariRoadDatasetImageAugget_transforms\n\n\n\n init getitemlen\n\n\ndef __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\ninit 메소드는 클래스가 초기화될 때 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,\n이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n\n\ndef __getitem__(self, idx):\n    img_file= self.img_files[idx].as_posix()\n    label_file = img_file.replace('images', 'labels')\n    img = cv2.imread(img_file)\n    label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n    img, label = self.transform(img, label)\n    return img, label, img_file\n\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n\n\n\n\ninitcall\n\n\ndef __init__(self, train):\n    if train:\n        self.aug = A.Compose([A.RandomCrop(256, 256),\n                                A.HorizontalFlip(p=0.5),\n                                A.ShiftScaleRotate(p=0.3),\n                                A.RandomBrightnessContrast(p=0.3),\n                                A.pytorch.transforms.ToTensorV2()])\n    else:\n        self.aug = ToTensorV2()\n\n\ndef __call__(self, img, label):\n    transformed = self.aug(image=img, mask=np.squeeze(label))\n    return transformed['image']/255.0, transformed['mask']\n\n\n\n\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\n\n\n\n\n\n코드\n\n---\ndef __init__(self, root, train=False):\n---\n\n\n\n\n\n\n결과\n\n---\n매번 클래스가 초기화될 때마다.\n1) 학습 모드에 따른 이미지 파일 경로설정 \n2) class 내에서 사용할 정보들을 정의\n---\n\n\n\n\n\nUtils\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\n\n\n make_color_label plot_image\n\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\ninit 메소드는 클래스가 초기화될 때 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,\n이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\ninit 메소드는 클래스가 초기화될 때 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,\n이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n\n\n\n\n\nConfusionMatrix\n\nclass ConfusionMatrix:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.confusion_matrix = np.zeros((num_classes, num_classes))\n        \n    def process_batch(self, preds, targets):\n        targets = targets.cpu().numpy().flatten()\n        preds = preds.argmax(1).cpu().numpy().flatten()\n        mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n        \n        confusion_mtx = np.bincount(\n                        self.num_classes * targets[mask].astype(int) + preds[mask],\n                        minlength=self.num_classes ** 2)\n        confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n        self.confusion_matrix += confusion_mtx\n        return confusion_mtx\n    \n    def print(self):\n        for i in range(self.num_classes):\n            print(f\"Class{i}:{self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n\n    def get_pix_acc(self):\n        return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n\n    def get_class_acc(self):\n        class_acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        return np.nanmean(class_acc),class_acc\n    \n    def get_iou(self):\n        divisor = self.confusion_matrix.sum(axis=1)\\\n                    + self.confusion_matrix.sum(axis=0)\\\n                    - np.diag(self.confusion_matrix)\n        iou = np.diag(self.confusion_matrix) / divisor\n        return iou\n    \n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\n \n\n\nConfusionMatrix사용예시\n\n\n\n init process_batchprintget_pix_accget_class_accget_iouget_mean_iou\n\n\ndef __init__(self, num_classes):\n    self.num_classes = num_classes\n    self.confusion_matrix = np.zeros((num_classes, num_classes))\ninit 메소드는 클래스가 초기화될 때 클래스 내에서 사용할 수 있도록 준비물을 준비하는 단계입니다.\n예시 상황\n class의 수를 4이라고 했을때, 0으로 구성된 4by4행렬이 준비된다.\n\n\n\n                   np.zeros((3,3))\n\n\n\n\ndef process_batch(self, preds, targets):\n    targets = targets.cpu().numpy().flatten()\n    preds = preds.argmax(1).cpu().numpy().flatten()\n    mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n    \n    confusion_mtx = np.bincount(\n                    self.num_classes * targets[mask].astype(int) + preds[mask],\n                    minlength=self.num_classes ** 2)\n    confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n    self.confusion_matrix += confusion_mtx\n[설명]\n\n\n\n이해를 돕기위해 사진의 targets과 preds을 process_batch에 적용해보려고 한다.\n\n\nmask는 사진에서 설정한 클래스(여기서는 4로 설정했으니깐 0~3) 값인 것만 남기고,\n그 외의 값은 제외하는 역할을 한다.\n\n\n\n정답traget.shape=(3,3) 추론값preds.shape=(3,3)을 flaten으로 일렬로 나열해 shape을 (9)로 만든다.\n\n\n\n\n\n[클래스*정답(9) + 추론값(9)]의 식을 이용해 인덱스를 계산한다. 결과=&gt; [0, 6, 10, 13, 10, 5, 0, 15, 4]\n\n\n\n\n\n[0, 6, 10, 13, 10, 5, 0, 15, 4]에서 0은 2개 1은 0개 15은 1개 이런식으로 bincount로 원소값의 갯수를 샌다. 갯수의 결과가 [2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1] 이렇게 나온다.  tip. minlength는 인덱스의 범위를 나타냄 이 식에서는 4x4 표를 만들것이기 때문에 4**2f로 설정\n\n\n\n\n\nconfusion_mtx.reshape(4, 4)를 통해 길이16벡터를 4by4 행렬로 변경 후, 방금 init에서 만들었던 0으로 구성된 4by4 self.confusion_matrix에 넣어준다.\n\n\n\n\ndef print(self):\n    for i in range(self.num_classes):\n        print(f\"Class{i}:\n                {self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n각 클래스별 accuracy를 표현\n\nClass0:2.0 / 2.0  &lt;=정답0 2개 중 2개를 맞췄어\nClass1:1.0 / 3.0  &lt;=정답1 3개 중 1개를 맞췄어\nClass2:2.0 / 2.0  &lt;=정답1 2개 중 2개를 맞췄어\nClass3:1.0 / 2.0  &lt;=정답1 2개 중 1개를 맞췄어\n\n\ndef get_pix_acc(self):\n    return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\nget_pix_acc 메서드는 픽셀 정확도(Pixel Accuracy)를 계산하는 역할 픽셀 정확도는 이미지 분할이나 분류 작업에서 예측된 픽셀이 실제 값과 일치하는 비율\n\n\n\n(np.diag행렬의 가운데값을 모두 더한값(6)) 나누기 (confusion_matrix.sum행렬의 모든값 더한값(9)) get_pix_acc답 : 6/9\n\n\n\n\ndef get_class_acc(self):\n    class_acc =np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n    return np.nanmean(class_acc),class_acc\n도로 종류(클래스)가 비포장도로 국도 고속도로로 3개의 클래스가 있다고 하자 도심에서 도로탐지를 한 뒤 평가하려고 하면 도시에 비포장도로가 없기에 accuracy를 구하면 비포장도로의 클래스는 0(Nan)값이 나오고 전체적 accuracy가 감소할 것이다. 이것을 방지하기 위해서 get_class_acc을 쓰는것이다. 방법은 np.nanmean()을 쓰면 된다.\n\n\ndef get_iou(self):\n    divisor = self.confusion_matrix.sum(axis=1)\\\n                + self.confusion_matrix.sum(axis=0)\\\n                - np.diag(self.confusion_matrix)\n    iou = np.diag(self.confusion_matrix) / divisor\n    return iou\n각 클래스별 iou를 구하는 코드이다. 출력값은 다음과 같이 나온다. &gt;&gt;IoU: [0.66666667 0.25 0.66666667 0.5 ]\n\n\n\n\n\niou 0.39 예시\n\n\n\n\n\n\niou는 Combined Region부분 나누기 Overlapping 이다.\n\n\nIOU는 예측한 픽셀과 실제 픽셀간 겹침 정도를 평가하는 지표이다.  사람이 눈으로 평가하는 지표와 가장 유사한 지표이다.  주로 객체 인식 알고리즘의 정확도를 평가하는데 사용된다.\n\n\n\n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\nget_iou의 평균값이다.  사진에 포함되지 않은 클래스가 있을 경우 iou값이 Nan값으로 나오게 된다.  Nan값을 제거하고 정확한 평균 계산을 위해 np.nansum()으로 더해준다.\n\n\n\n\n\nex_targets=torch.Tensor([[[0,1,2],[3,2,1],[0,3,1]]])#shape=(1,3,3)=(batchsize,H,W)\nex_preds=torch.Tensor([[[[1,0,0], \n                     [0,0,0],\n                     [1,0,1]],\n                    [[0,0,0],\n                     [1,0,1],\n                     [0,0,0]],\n                    [[0,1,1],\n                     [0,1,0],\n                     [0,0,0]],\n                    [[0,0,0],\n                     [0,0,0],\n                     [0,1,0]]]]) #shape=(1,4,3,3)=(batchsize,C,H,W)\n\n\n# ConfusionMatrix 클래스 인스턴스화\nnum_classes = 4  # 0 ~ 10 사이의 숫자를 사용하므로 총 11개의 클래스\nconfusion_matrix = ConfusionMatrix(num_classes)\n\n# 배치 처리\nconfusion_matrix.process_batch(ex_preds, ex_targets)\n\n# 메트릭 출력\nconfusion_matrix.print()\nprint(\"process_batch:\", confusion_matrix.process_batch(ex_preds, ex_targets))\nprint(\"Pixel Accuracy:\", confusion_matrix.get_pix_acc())\nprint(\"Class Accuracy:\", confusion_matrix.get_class_acc())\nprint(\"IoU:\", confusion_matrix.get_iou())\nprint(\"Mean IoU:\", confusion_matrix.get_mean_iou())\n\n\n결과\nClass0:2.0 / 2.0  &lt;=정답0 2개 중 2개를 맞췄어\nClass1:1.0 / 3.0  &lt;=정답1 3개 중 1개를 맞췄어\nClass2:2.0 / 2.0  &lt;=정답1 2개 중 2개를 맞췄어\nClass3:1.0 / 2.0  &lt;=정답1 2개 중 1개를 맞췄어\nprocess_batch:\n[[2 0 0 0]\n[1 1 1 0]\n[0 0 2 0]\n[0 1 0 1]]\nPixel Accuracy: 0.6666\nClass Accuracy: 0.7083\nIoU: [0.66666667 0.25 0.66666667 0.5 ]\nMean IoU: 0.52083\n\n\n\n\n\n\n\n\n\n\nLoss\n\ndef bce_loss(preds, targets, pos_weight=None):\n    bce_loss = F.binary_cross_entropy_with_logits(\n        preds.float(),\n        targets.float(),\n        pos_weight=pos_weight,\n    )\n    return bce_loss\n\n\ndef ce_loss(preds, targets, ignore=255):\n    ce_loss = F.cross_entropy(\n        preds.float(),\n        targets.long(),    # [B, H, W]\n        ignore_index=ignore,\n    )\n    return ce_loss\n\n\ndef dice_loss(preds, targets, eps=1e-7):\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)   # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)                        # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))        # dims = (0, 2, 3)\n    intersection = torch.sum(probas * true_1_hot, dims)     # intersection w.r.t. the class\n    cardinality = torch.sum(probas + true_1_hot, dims)      # cardinality w.r.t. the class\n    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n    return (1 - dice_loss)\n\n\ndef jaccard_loss(preds, targets, eps=1e-7):\n    \"\"\"Computes the Jaccard loss.\n    Args:\n    preds(logits) a tensor of shape [B, C, H, W]\n    targets: a tensor of shape [B, 1, H, W].\n    eps: added to the denominator for numerical stability.\n    Returns:\n        Jaccard loss\n    \"\"\"\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return (1 - jacc_loss)\n\n\n bce_loss ce_lossdice_lossjaccard_loss\n\n\n가중치가 적용된 이진 교차 엔트로피 손실을 계산\nArgs:\n    targets: [B, 1, H, W] 형태의 텐서. 실제 값 (레이블).\n    preds: [B, 1, H, W] 형태의 텐서. 모델의 예측 값.\n    pos_weight: 양성 클래스에 대한 가중치 (선택 사항).\n\nReturns:\n    bce_loss: 가중치가 적용된 이진 교차 엔트로피 손실.\n\n\n\nBCE LOSS\n\n\n\nN: 데이터의 수\ny𝑖𝑐 : 실제 레이블 (0 또는 1)\np𝑖𝑐 : 모델이 예측한 클래스 1에 속할 확률\n\n목적: BCE의 주된 목적은 이진 분류 모델의 예측값 𝑝𝑖가 실제 레이블 𝑦𝑖와 얼마나 잘 일치하는지를 측정하는 것입니다. 이 손실 함수는 예측값과 실제 레이블 간의 차이를 최소화하도록 모델을 학습시킵니다.\n손실 계산: BCE는 각 데이터 포인트에 대해 예측값 𝑝𝑖가 실제 레이블 𝑦𝑖와 얼마나 비슷한지를 측정합니다. 예측값이 실제 레이블과 정확히 일치할 때 손실은 0이 되고, 그렇지 않으면 손실이 증가합니다. 이를 통해 모델이 더 정확한 예측을 하도록 유도합니다.\n확률적 해석: BCE는 각 예측값 𝑝𝑖을 클래스 1에 속할 확률로 해석하며, 이를 로그 함수를 통해 손실을 계산합니다. 따라서 모델은 클래스 1에 속할 확률을 정확히 예측하도록 학습됩니다.\n최적화: BCE는 일반적으로 이진 분류 모델의 손실 함수로 사용되며, 경사 하강법을 통해 모델의 가중치를 조정하여 손실을 최소화하도록 합니다. 이를 통해 모델이 데이터의 패턴을 학습하고, 새로운 데이터에 대해 정확한 예측을 할 수 있도록 합니다.\n\n\n가중치가 적용된 다중 클래스 교차 엔트로피 손실을 계산\n\nArgs:\n    targets: [B, H, W] 형태의 텐서. 실제 값 (레이블).\n    preds: [B, C, H, W] 형태의 텐서. 모델의 예측 값.\n    ignore: 무시할 클래스 인덱스.\n\nReturns:\n    ce_loss: 가중치가 적용된 다중 클래스 교차 엔트로피 손실.\n\n\n\nCE LOSS\n\n\n\nN: 데이터의 수\nC: 클래스의 수\ny𝑖𝑐 : 데이터 포인트 i의 실제 클래스 𝑐에 대한 one-hot 인코딩된 레이블 (0 또는 1)\np𝑖𝑐 : 모델이 예측한 데이터 포인트 𝑖가 클래스 c에 속할 확률\n\n목적: CE 손실 함수는 다중 클래스 분류 모델에서 예측값 pic가 실제 레이블yic와 얼마나 일치하는지를 측정합니다. 모델이 다양한 클래스를 분류하고, 각 클래스에 속할 확률을 정확하게 예측하도록 학습시키는 데 사용됩니다.\n손실 계산: 각 데이터 포인트에 대해 CE는 각 클래스에 대해 예측된 확률 pic가 실제 레이블 yic 사이의 교차 엔트로피를 계산하여 전체 데이터셋에 대한 평균 손실을 계산합니다. 이 손실은 모델이 예측을 향상시키고, 다양한 클래스 간의 결정 경계를 명확하게 만들도록 돕습니다.\n확률적 해석: CE 손실 함수는 예측된 확률 pic을 클래스 c에 속할 확률로 해석하며, 이를 로그 함수를 통해 손실을 계산합니다. 따라서 모델은 각 데이터 포인트가 각 클래스에 속할 가능성을 정확히 예측하도록 학습됩니다.\n최적화: CE 손실 함수는 다중 클래스 분류 문제에서 모델을 훈련시키는 데 주로 사용됩니다. 경사 하강법을 통해 모델의 가중치를 조정하여 CE 손실을 최소화하도록 하여 모델의 성능을 향상시킵니다.\n\n\nSørensen–Dice 손실을 계산합니다.\n\nArgs:\n    preds(logits): [B, C, H, W] 형태의 텐서. 모델의 예측 값 (로짓).\n    targets: [B, 1, H, W] 형태의 텐서. 실제 값 (레이블).\n    eps: 분모에 더해지는 작은 값으로, 수치적 안정성을 위해 사용됩니다.\n\nReturns:\n    dice_loss: Sørensen–Dice 손실 값.\n\n\n\nDICE LOSS\n\n\n\n∣P∩T∣: 예측된 세그멘테이션 영역 P과 실제 타겟 세그멘테이션 영역 T의 교집합의 크기입니다.\n∣P∣: 예측된 세그멘테이션 영역 P의 크기 또는 원소 수입니다.\n∣T∣: 실제 타겟 세그멘테이션 영역 T의 크기 또는 원소 수입니다.\n\n목적: Dice 계수는 두 세그멘테이션 영역의 중첩 정도를 측정하여 예측된 세그멘테이션 𝑃이 실제 타겟 세그멘테이션 T과 얼마나 일치하는지를 평가합니다. 이 지표는 0에서 1 사이의 값을 가지며, 1에 가까울수록 두 영역이 완전히 일치함을 나타냅니다.\n계산 방법: Dice 계수의 분자 2×∣𝑃∩𝑇∣는 예측된 영역과 실제 타겟 영역의 교집합의 크기를 두 배로 확장하여, under-segmentation과 over-segmentation을 모두 공평하게 패널티를 줍니다. 분모 ∣P∣+∣T∣는 두 영역의 크기 합으로 정규화하여 계수가 [0, 1] 범위 내에 유지되도록 합니다.\n응용: Dice 계수는 주로 의료 이미지 분석 및 다양한 영상 처리 애플리케이션에서 사용됩니다. 이미지 세분화 알고리즘의 정확도를 평가하고, 예측된 세그멘테이션의 질을 비교하는 데 중요한 지표 역할을 합니다.\n\n\n    Jaccard 손실을 계산합니다.\n    인수:\n    preds(logits): 형상이 [B, C, H, W]인 텐서\n    targets: 형상이 [B, 1, H, W]인 텐서\n    eps: 수치 안정성을 위해 분모에 추가되는 작은 값\n    반환:\n    Jaccard 손실\n\n\n\nJACCARD LOSS\n\n\n\n∣P∩T∣: 예측된 영역 𝑃과 실제 타겟 영역 𝑇의 교집합의 크기\n∣P∪T∣: 예측된 영역 𝑃과 실제 타겟 영역 𝑇의 합집합의 크기\n\n목적: Jaccard 지수는 두 세그멘테이션 영역의 중첩 정도를 측정하여 예측된 세그멘테이 P이 실제 타겟 세그멘테이션 T과 얼마나 일치하는지를 평가합니다. 이 지표는 0에서 1 사이의 값을 가지며, 1에 가까울수록 두 영역이 완전히 일치함을 나타냅니다.\n계산 방법: Jaccard 지수는 예측된 영역 𝑃과 실제 타겟 영역 𝑇의 교집합 크기를 두 영역의 합집합 크기로 나누어 계산합니다. 이는 두 영역이 얼마나 겹치는지를 평가하며, 분자는 교집합의 크기를 나타내고 분모는 전체적인 예측된 영역과 실제 타겟 영역의 크기를 반영합니다.\n응용: Jaccard 지수는 주로 이미지 세분화의 성능을 평가하는 데 사용됩니다. 다양한 이미지 처리 및 패턴 인식 문제에서 유사성을 비교하고 모델의 정확도를 평가하는 데 중요한 지표로 활용됩니다.\n\n\n\n\n\none_epoch( train, val )\n\ndef train_one_epoch(train_dataloader, model, optimizer, device):\n    model.train()\n    losses = [] \n    for i, (imgs, targets, _) in enumerate(train_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        preds = model(imgs)['out']     # forward \n        loss = ce_loss(preds, targets) # calculates the iteration loss  \n        optimizer.zero_grad()   # zeros the parameter gradients\n        loss.backward()         # backward\n        optimizer.step()        # update weights\n        print('\\t iteration: %d/%d, loss=%.4f' % (i, len(train_dataloader)-1, loss))    \n        losses.append(loss.item())\n    return torch.tensor(losses).mean().item()\n\n\ndef val_one_epoch(val_dataloader, model, confusion_matrix, device):\n    model.eval()\n    losses = []\n    for i, (imgs, targets, img_file) in enumerate(val_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        with torch.no_grad():\n            preds = model(imgs)['out']   # forward, preds: (B, 2, H, W)\n            loss = ce_loss(preds, targets)\n            losses.append(loss.item())\n            confusion_matrix.process_batch(preds, targets)\n            # sample images\n            if i == 0:\n                preds = torch.argmax(preds, axis=1) # (1, H, W)  \n                for j in range(3):\n                    save_file = os.path.join('outputs', 'val_%d.png' % (j))\n                    plot_image(imgs[j], preds[j], save_file)\n                \n    avg_loss = torch.tensor(losses).mean().item()\n    \n    return avg_loss\n\n\ntrain_one_epochval_one_epoch\n\n\n1) def train_one_epoch(train_dataloader, model, optimizer, device)이라는\n    함수는 파라미터가 4개이다.\n2) model.train()은 학습모드인것을 나타낸다.\n3) losses = []는 train의 loss값을 저장하기 위함이다.\n4) 반복문(for)의 i 는 enumerate함수에서 만들어진(인덱싱된) 0부터 n까지의\n    숫자이고 (imgs, targets, _)는 각각 imgs는 위성사진 targets는 정답라벨이다.\n5) device를 통해 cpu에서 gpu로 이동시키는 과정으로 device는 뒤에서 정의할 것이다.\n6) model에 img를 집어넣어 preds를 예측해본다. 이때 [out]이 붙는 이유는\n    torchvision에 저장되어 있는 deeplabv3모델을 사용할때의 방식이다.\n7) loss는 앞에서 정의해준 로스 중 ce_loss를 사용했다. \n    이때 preds가 먼저 들어가야하는 점을 주의하자\n8) optimizer.zero는 loss의 미분값을 계산하기전 0으로 초기화하는 과정이다.\n9) loss.backward()는 loss값을 편미분하는 과정이다. \n    편미분은 2차원 공간에서 미분을 한 번에 못하니깐 \n    순차적으로 미분하는 과정이라고 생각하면 된다.\n10) 미분한 값을 모델의 파라미터값에 반영한다(미분값을 빼주는과정). \n11) iteration별 train loss를 표시해준다.\n12) loss를 losses에 저장한다.\n13) return을 통해 함수 밖에서도 losses의 평균을 호출 할 수 있게한다.\n\n\n1) def val_one_epoch(val_dataloader, model, confusion_matrix, device)이라는\n    함수는 파라미터가 4개이다.\n2) model.eval()은 학습모드인것을 나타낸다.\n3) losses = []는 validation의 loss값을 저장하기 위함이다.\n4) 반복문(for)의 i 는 enumerate함수에서 만들어진(인덱싱된) 0부터 n까지의\n    숫자이고 (imgs, targets, _)는 각각 imgs는 위성사진 targets는 정답라벨이다.\n5) device를 통해 cpu에서 gpu로 이동시키는 과정으로 device는 뒤에서 정의할 것이다.\n6) model에 img를 집어넣어 preds를 예측해본다. 이때 [out]이 붙는 이유는\n    torchvision에 저장되어 있는 deeplabv3모델을 사용할때의 방식이다.\n7) loss는 앞에서 정의해준 로스 중 ce_loss를 사용했다. \n    이때 preds가 먼저 들어가야하는 점을 주의하자\n8) loss를 losses에 저장한다.\n9) 이후에 ConfusionMatrix를 이용해 confusion_matrix라는 인스턴스를 만들게\n 되는데 만들어진 인스턴스에 모델이 예측한 preds와 정답지 targets을 넣어주면 \n 행렬이 계산이 돼서 아래와 같이 사용가능하다.\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc() \n\n12) ~ 16) i=0일때(epoch이 새로 시작될때)마다 plot_image 함수를 이용해 \nimg에 preds를 오버레이해서 사진으로 저장한다.(총 3장 저장한다.) \n추가로 저장된 사진은 이름이 val_0(1,2).png으로 매 에폭마다 사진이 바뀐다. \n따로 저장하려면 수정필요\n17~) return을 통해 함수 밖에서도 validation losses의 평균을 호출 할 수 있게한다.\n\n\n\n\n\nTrain\n\n#def train______________________________________________________#   \ndef train(epochs=200, batch_size = 8 , name = 'suwany'):\n#1______________________________________________________________#    \n    # wandb settings\n    wandb.init(id=name, resume='allow', mode='disabled')\n    wandb.config.update({\n        'epochs': epochs,\n        'batch_size': batch_size,\n        'name': name\n    })\n#2______________________________________________________________#   \n    # Train dataset\n    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n    # Train dataloader\n    num_workers = min([os.cpu_count(), batch_size, 16])\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    # Validation dataset\n    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n#3______________________________________________________________#   \n    # Network model\n    num_classes = 10 # background + 1 classes\n    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n    \n    # GPU-support\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.device_count() &gt; 1:   # multi-GPU\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n#______________________________________________________________#   \n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n      \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n#______________________________________________________________#   \n    # loading a weight file (if exists)\n    weight_file = Path('weights')/(name + '.pth')\n    best_accuracy = 0.0\n    start_epoch, end_epoch = (0, epochs)\n    if os.path.exists(weight_file):\n        checkpoint = torch.load(weight_file)\n        model.load_state_dict(checkpoint['model'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_accuracy = checkpoint['best_accuracy']\n        print('resumed from epoch %d' % start_epoch)\n#______________________________________________________________#       \n    \n    confusion_matrix = ConfusionMatrix(num_classes)\n # training/validation\n    for epoch in range(start_epoch, end_epoch):\n        print('epoch: %d/%d' % (epoch, end_epoch-1))\n        t0 = time.time()\n        # training\n        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n        t1 = time.time()\n        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n        lr_scheduler.step(epoch_loss)\n        # validation\n        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n        \n        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n        # saving the best status into a weight file\n        if val_epoch_pix_accuracy &gt; best_accuracy:\n             best_weight_file = Path('weights')/(name + '_best.pth')\n             best_accuracy = val_epoch_pix_accuracy\n             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n             torch.save(state, best_weight_file)\n             print('best accuracy=&gt;saved\\n')\n        # saving the current status into a weight file\n        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n        torch.save(state, weight_file)\n        # wandb logging\n        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n#______________________________________________________________#          \n\n\n함수 train 소개def trainwandbdataset, dataloadernum_classes, model, device Optimizer,lr_scheduler loadtrain_start\n\n\n이 함수는 지금까지 정의한 함수들을 이용함, 주어진 하이퍼파라미터 설정을 기반으로 모델을 학습하고 검증하는 과정을 포함되어있다. 또한, 모델의 상태를 저장하고, WandB(Weights and Biases)를 사용하여 로그를 기록한다.\n\n\ndef train(epochs=200, batch_size = 8 , name = 'suwany'):\n\n\nepochs: 학습할 총 에포크 수입니다. 기본값은 200입니다.\nbatch_size: 미니배치의 크기입니다. 기본값은 8입니다.\nname: 실험 이름입니다. 기본값은 ’suwany’입니다.\n\n\n\n    wandb.init(id=name, resume='allow', mode='disabled')\n    wandb.config.update({\n        'epochs': epochs,\n        'batch_size': batch_size,\n        'name': name\n    })\n\nWandB를 초기화하고 설정을 업데이트합니다. 이로 인해 실험의 하이퍼파라미터가 기록됩니다\n\n\n\n    # Train dataset\n    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n    # Train dataloader\n    num_workers = min([os.cpu_count(), batch_size, 16])\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    # Validation dataset\n    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n\nKariRoadDataset 클래스를 사용하여 학습 및 검증 데이터셋을 초기화합니다.\n각 데이터셋에 대해 데이터로더를 생성하여 데이터를 배치 단위로 로드합니다.\nnum_workers는 데이터 로딩을 위한 CPU 스레드 수를 설정합니다.\n\n\n\n    num_classes = 10  # background + 9 classes\n    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n    \n    # GPU-support\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.device_count() &gt; 1:   # multi-GPU\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n\nnum_classes를 설정하여 모델의 클래스 수를 정의한다.\n\n여기서는 road의 클래스 9개에 배경 1개를 추가한다.\n\nDeepLabV3 모델을 ResNet-101 백본을 사용하여 초기화했다.\n모델을 GPU로 이동시키며, 여러 GPU를 사용할 경우 DataParallel을 사용하도록 했다.\n\n\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n      \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n\nAdam 옵티마이저를 설정하여 모델 파라미터를 최적화한다.\n학습률 스케줄러는 검증 손실이 5회 이상 줄어들지 않을 때 학습률을 감소시키도록 설정했다.\n\n\n\n    weight_file = Path('weights')/(name + '.pth')\n    best_accuracy = 0.0\n    start_epoch, end_epoch = (0, epochs)\n    if os.path.exists(weight_file):\n        checkpoint = torch.load(weight_file)\n        model.load_state_dict(checkpoint['model'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_accuracy = checkpoint['best_accuracy']\n        print('resumed from epoch %d' % start_epoch)\n\n기존에 저장된 체크포인트 파일이 있으면 이를 로드하여 학습을 이어서 진행할 수 있다.\n\n\n    confusion_matrix = ConfusionMatrix(num_classes)\n    # training/validation\n    for epoch in range(start_epoch, end_epoch):\n        print('epoch: %d/%d' % (epoch, end_epoch-1))\n        t0 = time.time()\n        # training\n        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n        t1 = time.time()\n        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n        lr_scheduler.step(epoch_loss)\n        # validation\n        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n        \n        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n        # saving the best status into a weight file\n        if val_epoch_pix_accuracy &gt; best_accuracy:\n             best_weight_file = Path('weights')/(name + '_best.pth')\n             best_accuracy = val_epoch_pix_accuracy\n             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n             torch.save(state, best_weight_file)\n             print('best accuracy=&gt;saved\\n')\n        # saving the current status into a weight file\n        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n        torch.save(state, weight_file)\n        # wandb logging\n        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n\n\nfor문을 이용해 각 에포크마다 학습 및 검증을 수행한다.\ntrain_one_epoch 함수는 한 에포크 동안 학습을 수행하며, 손실 값을 반환했다.\n검증 단계에서는 val_one_epoch 함수를 통해 검증 손실, IoU, 평균 IoU, 픽셀 정확도를 계산한다.\n최상의 모델 상태를 저장하며, 현재 상태도 에폭마다 저장되도록 했다.\nWandB를 사용하여 학습 및 검증 손실, 정확도를 로깅되어 기록된다.\n\n\n\n\n\n\n실행 코드\n\ntrain(epochs=200, batch_size = 8 , name = 'suwany')\n\nepoch: 0/199\n     iteration: 0/142, loss=2.3710\n     iteration: 1/142, loss=2.3142\n     iteration: 2/142, loss=2.2351\n     iteration: 3/142, loss=2.2264\n     iteration: 4/142, loss=2.1433\n     iteration: 5/142, loss=2.0343\n     iteration: 6/142, loss=1.9032",
    "crumbs": [
      "About",
      "Posts",
      "Post with Code",
      "Road_segmantation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "semantic-segmentation",
    "section": "",
    "text": "개요\nImage semantic-segmentation using kari-dataset The goal of this project is to label every pixel in an image, dividing it into meaningful segments.\n\nThis work also references code from Dr. Ohhan.\n목차\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 25, 2024\n\n\nRoad_segmantation\n\n\n \n\n\n\n\n\nNo matching items"
  }
]