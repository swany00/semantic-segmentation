[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog",
    "crumbs": [
      "About"
    ]
  },
  {
    "objectID": "posts/post-with-code/road_seg.html",
    "href": "posts/post-with-code/road_seg.html",
    "title": "Road_segmantation",
    "section": "",
    "text": "This is a post with executable code.\n\nImports\n\n#Datasets\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n#Utils\nimport os\nimport cv2\nimport numpy as np\nimport torch\n\n#ConfusionMatrix\nimport numpy as np\n\n#Loss\nimport torch\nimport torch.nn.functional as F\n\n#Train\nimport os\nimport argparse\nimport torch\nimport torch.optim as optim\nimport time\nimport wandb\nfrom pathlib import Path\nfrom torchvision import models\n\n\n\nDataset\n\nclass KariRoadDataset(torch.utils.data.Dataset):\n    def __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\n\n    def __getitem__(self, idx):\n        img_file= self.img_files[idx].as_posix()\n        label_file = img_file.replace('images', 'labels')\n        img = cv2.imread(img_file)\n        label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n        img, label = self.transform(img, label)\n        return img, label, img_file\n\n    def __len__(self):\n        return len(self.img_files)\n    \nclass ImageAug:\n    def __init__(self, train):\n        if train:\n            self.aug = A.Compose([A.RandomCrop(256, 256),\n                                  A.HorizontalFlip(p=0.5),\n                                  A.ShiftScaleRotate(p=0.3),\n                                  A.RandomBrightnessContrast(p=0.3),\n                                  A.pytorch.transforms.ToTensorV2()])\n        else:\n            self.aug = ToTensorV2()\n\n    def __call__(self, img, label):\n        transformed = self.aug(image=img, mask=np.squeeze(label))\n        return transformed['image']/255.0, transformed['mask']\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\nKariRoadDatasetImageAugget_transforms\n\n\n\n init getitemlen\n\n\ndef __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\ninit 메소드는 클래스가 초기화될 때 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,\n이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n\n\ndef __getitem__(self, idx):\n    img_file= self.img_files[idx].as_posix()\n    label_file = img_file.replace('images', 'labels')\n    img = cv2.imread(img_file)\n    label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n    img, label = self.transform(img, label)\n    return img, label, img_file\n\n\n    def __len__(self):\n        return len(self.img_files)\n\n\n\n\n\n\ninitcall\n\n\ndef __init__(self, train):\n    if train:\n        self.aug = A.Compose([A.RandomCrop(256, 256),\n                                A.HorizontalFlip(p=0.5),\n                                A.ShiftScaleRotate(p=0.3),\n                                A.RandomBrightnessContrast(p=0.3),\n                                A.pytorch.transforms.ToTensorV2()])\n    else:\n        self.aug = ToTensorV2()\n\n\ndef __call__(self, img, label):\n    transformed = self.aug(image=img, mask=np.squeeze(label))\n    return transformed['image']/255.0, transformed['mask']\n\n\n\n\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\n\n\n\n\n\n코드\n\n---\ndef __init__(self, root, train=False):\n---\n\n\n\n\n\n\n결과\n\n---\n매번 클래스가 초기화될 때마다.\n1) 학습 모드에 따른 이미지 파일 경로설정 \n2) class 내에서 사용할 정보들을 정의\n---\n\n\n\n\n\nUtils\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\n\n\n make_color_label plot_image\n\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\ninit 메소드는 클래스가 초기화될 때 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,\n이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\ninit 메소드는 클래스가 초기화될 때 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,\n이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n\n\n\n\n\nConfusionMatrix\n\nclass ConfusionMatrix:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.confusion_matrix = np.zeros((num_classes, num_classes))\n        \n    def process_batch(self, preds, targets):\n        targets = targets.cpu().numpy().flatten()\n        preds = preds.argmax(1).cpu().numpy().flatten()\n        mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n        \n        confusion_mtx = np.bincount(\n                        self.num_classes * targets[mask].astype(int) + preds[mask],\n                        minlength=self.num_classes ** 2)\n        confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n        self.confusion_matrix += confusion_mtx\n        return confusion_mtx\n    \n    def print(self):\n        for i in range(self.num_classes):\n            print(f\"Class{i}:{self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n\n    def get_pix_acc(self):\n        return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n\n    def get_class_acc(self):\n        class_acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        return np.nanmean(class_acc),class_acc\n    \n    def get_iou(self):\n        divisor = self.confusion_matrix.sum(axis=1)\\\n                    + self.confusion_matrix.sum(axis=0)\\\n                    - np.diag(self.confusion_matrix)\n        iou = np.diag(self.confusion_matrix) / divisor\n        return iou\n    \n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\n \n\n\nConfusionMatrix사용예시\n\n\n\n init process_batchprintget_pix_accget_class_accget_iouget_mean_iou\n\n\ndef __init__(self, num_classes):\n    self.num_classes = num_classes\n    self.confusion_matrix = np.zeros((num_classes, num_classes))\ninit 메소드는 클래스가 초기화될 때 클래스 내에서 사용할 수 있도록 준비물을 준비하는 단계입니다.\n예시 상황\n class의 수를 4이라고 했을때, 0으로 구성된 4by4행렬이 준비된다.\n\n\n\n                   np.zeros((3,3))\n\n\n\n\ndef process_batch(self, preds, targets):\n    targets = targets.cpu().numpy().flatten()\n    preds = preds.argmax(1).cpu().numpy().flatten()\n    mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n    \n    confusion_mtx = np.bincount(\n                    self.num_classes * targets[mask].astype(int) + preds[mask],\n                    minlength=self.num_classes ** 2)\n    confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n    self.confusion_matrix += confusion_mtx\n[설명]\n\n\n\n이해를 돕기위해 사진의 targets과 preds을 process_batch에 적용해보려고 한다.\n\n\nmask는 사진에서 설정한 클래스(여기서는 4로 설정했으니깐 0~3) 값인 것만 남기고,\n그 외의 값은 제외하는 역할을 한다.\n\n\n\n정답traget.shape=(3,3) 추론값preds.shape=(3,3)을 flaten으로 일렬로 나열해 shape을 (9)로 만든다.\n\n\n\n\n\n[클래스*정답(9) + 추론값(9)]의 식을 이용해 인덱스를 계산한다. 결과=&gt; [0, 6, 10, 13, 10, 5, 0, 15, 4]\n\n\n\n\n\n[0, 6, 10, 13, 10, 5, 0, 15, 4]에서 0은 2개 1은 0개 15은 1개 이런식으로 bincount로 원소값의 갯수를 샌다. 갯수의 결과가 [2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1] 이렇게 나온다.  tip. minlength는 인덱스의 범위를 나타냄 이 식에서는 4x4 표를 만들것이기 때문에 4**2f로 설정\n\n\n\n\n\nconfusion_mtx.reshape(4, 4)를 통해 길이16벡터를 4by4 행렬로 변경 후, 방금 init에서 만들었던 0으로 구성된 4by4 self.confusion_matrix에 넣어준다.\n\n\n\n\ndef print(self):\n    for i in range(self.num_classes):\n        print(f\"Class{i}:\n                {self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n각 클래스별 accuracy를 표현\n\nClass0:2.0 / 2.0  &lt;=정답0 2개 중 2개를 맞췄어\nClass1:1.0 / 3.0  &lt;=정답1 3개 중 1개를 맞췄어\nClass2:2.0 / 2.0  &lt;=정답1 2개 중 2개를 맞췄어\nClass3:1.0 / 2.0  &lt;=정답1 2개 중 1개를 맞췄어\n\n\ndef get_pix_acc(self):\n    return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\nget_pix_acc 메서드는 픽셀 정확도(Pixel Accuracy)를 계산하는 역할 픽셀 정확도는 이미지 분할이나 분류 작업에서 예측된 픽셀이 실제 값과 일치하는 비율\n\n\n\n(np.diag행렬의 가운데값을 모두 더한값(6)) 나누기 (confusion_matrix.sum행렬의 모든값 더한값(9)) get_pix_acc답 : 6/9\n\n\n\n\ndef get_class_acc(self):\n    class_acc =np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n    return np.nanmean(class_acc),class_acc\n도로 종류(클래스)가 비포장도로 국도 고속도로로 3개의 클래스가 있다고 하자 도심에서 도로탐지를 한 뒤 평가하려고 하면 도시에 비포장도로가 없기에 accuracy를 구하면 비포장도로의 클래스는 0(Nan)값이 나오고 전체적 accuracy가 감소할 것이다. 이것을 방지하기 위해서 get_class_acc을 쓰는것이다. 방법은 np.nanmean()을 쓰면 된다.\n\n\ndef get_iou(self):\n    divisor = self.confusion_matrix.sum(axis=1)\\\n                + self.confusion_matrix.sum(axis=0)\\\n                - np.diag(self.confusion_matrix)\n    iou = np.diag(self.confusion_matrix) / divisor\n    return iou\n\n\n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\n\n\n\n\n\nex_targets=torch.Tensor([[[0,1,2],[3,2,1],[0,3,1]]])#shape=(1,3,3)=(batchsize,H,W)\nex_preds=torch.Tensor([[[[1,0,0], \n                     [0,0,0],\n                     [1,0,1]],\n                    [[0,0,0],\n                     [1,0,1],\n                     [0,0,0]],\n                    [[0,1,1],\n                     [0,1,0],\n                     [0,0,0]],\n                    [[0,0,0],\n                     [0,0,0],\n                     [0,1,0]]]]) #shape=(1,4,3,3)=(batchsize,C,H,W)\n\n\n# ConfusionMatrix 클래스 인스턴스화\nnum_classes = 4  # 0 ~ 10 사이의 숫자를 사용하므로 총 11개의 클래스\nconfusion_matrix = ConfusionMatrix(num_classes)\n\n# 배치 처리\nconfusion_matrix.process_batch(ex_preds, ex_targets)\n\n# 메트릭 출력\nconfusion_matrix.print()\nprint(\"process_batch:\", confusion_matrix.process_batch(ex_preds, ex_targets))\nprint(\"Pixel Accuracy:\", confusion_matrix.get_pix_acc())\nprint(\"Class Accuracy:\", confusion_matrix.get_class_acc())\nprint(\"IoU:\", confusion_matrix.get_iou())\nprint(\"Mean IoU:\", confusion_matrix.get_mean_iou())\n\n\n결과\nClass0:2.0 / 2.0  &lt;=정답0 2개 중 2개를 맞췄어\nClass1:1.0 / 3.0  &lt;=정답1 3개 중 1개를 맞췄어\nClass2:2.0 / 2.0  &lt;=정답1 2개 중 2개를 맞췄어\nClass3:1.0 / 2.0  &lt;=정답1 2개 중 1개를 맞췄어\nprocess_batch:\n[[2 0 0 0]\n[1 1 1 0]\n[0 0 2 0]\n[0 1 0 1]]\nPixel Accuracy: 0.6666\nClass Accuracy: 0.7083\nIoU: [0.66666667 0.25 0.66666667 0.5 ]\nMean IoU: 0.52083\n\n\n\n\n\n\n\n\n\n\nLoss\n\ndef bce_loss(preds, targets, pos_weight=None):\n    \"\"\"Computes the weighted binary cross-entropy loss.\n    Args:\n        targets: a tensor of shape [B, 1, H, W].\n        preds: a tensor of shape [B, 1, H, W]\n    Returns:\n        bce_loss: the weighted binary cross-entropy loss.\n    \"\"\"\n    bce_loss = F.binary_cross_entropy_with_logits(\n        preds.float(),\n        targets.float(),\n        pos_weight=pos_weight,\n    )\n    return bce_loss\n\n\ndef ce_loss(preds, targets, ignore=255):\n    \"\"\"Computes the weighted multi-class cross-entropy loss.\n    Args:\n        targets: a tensor of shape [B, H, W].\n        preds: a tensor of shape [B, C, H, W]. \n        ignore: the class index to ignore.\n    Returns:\n        ce_loss: the weighted multi-class cross-entropy loss.\n    \"\"\"\n    ce_loss = F.cross_entropy(\n        preds.float(),\n        targets.long(),    # [B, H, W]\n        ignore_index=ignore,\n    )\n    return ce_loss\n\n\ndef dice_loss(preds, targets, eps=1e-7):\n    \"\"\"Computes the Sørensen–Dice loss.\n    Args:\n    preds(logits) a tensor of shape [B, C, H, W]\n    targets: a tensor of shape [B, 1, H, W].\n    eps: added to the denominator for numerical stability.\n    Returns:\n        dice_loss: the Sørensen–Dice loss.\n    \"\"\"\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)   # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)                        # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))        # dims = (0, 2, 3)\n    intersection = torch.sum(probas * true_1_hot, dims)     # intersection w.r.t. the class\n    cardinality = torch.sum(probas + true_1_hot, dims)      # cardinality w.r.t. the class\n    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n    return (1 - dice_loss)\n\n\ndef jaccard_loss(preds, targets, eps=1e-7):\n    \"\"\"Computes the Jaccard loss.\n    Args:\n    preds(logits) a tensor of shape [B, C, H, W]\n    targets: a tensor of shape [B, 1, H, W].\n    eps: added to the denominator for numerical stability.\n    Returns:\n        Jaccard loss\n    \"\"\"\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return (1 - jacc_loss)\n\n\n\none_epoch( train, val )\n\ndef train_one_epoch(train_dataloader, model, optimizer, device):\n    model.train()\n    losses = [] \n    for i, (imgs, targets, _) in enumerate(train_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        preds = model(imgs)['out']     # forward \n        loss = ce_loss(preds, targets) # calculates the iteration loss  \n        optimizer.zero_grad()   # zeros the parameter gradients\n        loss.backward()         # backward\n        optimizer.step()        # update weights\n        print('\\t iteration: %d/%d, loss=%.4f' % (i, len(train_dataloader)-1, loss))    \n        losses.append(loss.item())\n    return torch.tensor(losses).mean().item()\n\n\ndef val_one_epoch(val_dataloader, model, confusion_matrix, device):\n    model.eval()\n    losses = []\n    total = 0\n    for i, (imgs, targets, img_file) in enumerate(val_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        with torch.no_grad():\n            preds = model(imgs)['out']   # forward, preds: (B, 2, H, W)\n            loss = ce_loss(preds, targets)\n            losses.append(loss.item())\n            confusion_matrix.process_batch(preds, targets)\n            total += preds.size(0)\n            # sample images\n            if i == 0:\n                preds = torch.argmax(preds, axis=1) # (1, H, W)  \n                for j in range(3):\n                    save_file = os.path.join('outputs', 'val_%d.png' % (j))\n                    plot_image(imgs[j], preds[j], save_file)\n                \n    avg_loss = torch.tensor(losses).mean().item()\n    \n    return avg_loss\n\n\n\nTrain\n\ndef train(epochs=200, batch_size = 8 , name = 'suwany'):\n    # wandb settings\n    wandb.init(id=name, resume='allow', mode='disabled')\n    wandb.config.update({\n        'epochs': epochs,\n        'batch_size': batch_size,\n        'name': name\n    })\n    \n    # Train dataset\n    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n    # Train dataloader\n    num_workers = min([os.cpu_count(), batch_size, 16])\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    # Validation dataset\n    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n    \n    # Network model\n    num_classes = 10 # background + 1 classes\n    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n    \n    # GPU-support\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.device_count() &gt; 1:   # multi-GPU\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n      \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n    # loading a weight file (if exists)\n    weight_file = Path('weights')/(name + '.pth')\n    best_accuracy = 0.0\n    start_epoch, end_epoch = (0, epochs)\n    if os.path.exists(weight_file):\n        checkpoint = torch.load(weight_file)\n        model.load_state_dict(checkpoint['model'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_accuracy = checkpoint['best_accuracy']\n        print('resumed from epoch %d' % start_epoch)\n    \n    \n    confusion_matrix = ConfusionMatrix(num_classes)\n # training/validation\n    for epoch in range(start_epoch, end_epoch):\n        print('epoch: %d/%d' % (epoch, end_epoch-1))\n        t0 = time.time()\n        # training\n        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n        t1 = time.time()\n        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n        lr_scheduler.step(epoch_loss)\n        # validation\n        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n        \n        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n        # saving the best status into a weight file\n        if val_epoch_pix_accuracy &gt; best_accuracy:\n             best_weight_file = Path('weights')/(name + '_best.pth')\n             best_accuracy = val_epoch_pix_accuracy\n             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n             torch.save(state, best_weight_file)\n             print('best accuracy=&gt;saved\\n')\n        # saving the current status into a weight file\n        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n        torch.save(state, weight_file)\n        # wandb logging\n        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n        \n\n\n\n실행 코드\n\ntrain(epochs=200, batch_size = 8 , name = 'suwany')\n\nERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n\n\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 train(epochs=200, batch_size = 8 , name = 'suwany')\n\nCell In[9], line 14, in train(epochs, batch_size, name)\n     12 # Train dataloader\n     13 num_workers = min([os.cpu_count(), batch_size, 16])\n---&gt; 14 train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n     15                         shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n     17 # Validation dataset\n     18 val_dataset = KariRoadDataset('./data/kari-road', train=False)\n\nFile ~/anaconda3/envs/gd/lib/python3.10/site-packages/torch/utils/data/dataloader.py:353, in DataLoader.__init__(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\n    351 else:  # map-style\n    352     if shuffle:\n--&gt; 353         sampler = RandomSampler(dataset, generator=generator)  # type: ignore[arg-type]\n    354     else:\n    355         sampler = SequentialSampler(dataset)  # type: ignore[arg-type]\n\nFile ~/anaconda3/envs/gd/lib/python3.10/site-packages/torch/utils/data/sampler.py:107, in RandomSampler.__init__(self, data_source, replacement, num_samples, generator)\n    103     raise TypeError(\"replacement should be a boolean value, but got \"\n    104                     \"replacement={}\".format(self.replacement))\n    106 if not isinstance(self.num_samples, int) or self.num_samples &lt;= 0:\n--&gt; 107     raise ValueError(\"num_samples should be a positive integer \"\n    108                      \"value, but got num_samples={}\".format(self.num_samples))\n\nValueError: num_samples should be a positive integer value, but got num_samples=0",
    "crumbs": [
      "About",
      "Posts",
      "Post with Code",
      "Road_segmantation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "semantic-segmentation",
    "section": "",
    "text": "개요\nImage semantic-segmentation using kari-dataset The goal of this project is to label every pixel in an image, dividing it into meaningful segments.\n\nThis work also references code from Dr. Ohhan.\n목차\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 25, 2024\n\n\nRoad_segmantation\n\n\n \n\n\n\n\n\nNo matching items"
  }
]