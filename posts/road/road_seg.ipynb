{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: \"Road_segmantation\"\n",
    "date: \"2024-06-25\"\n",
    "categories: [code, analysis]\n",
    "image: \"image.jpg\"\n",
    "---\n",
    "\n",
    "This is a post with executable code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Datasets\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "#Utils\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "#ConfusionMatrix\n",
    "import numpy as np\n",
    "\n",
    "#Loss\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Train\n",
    "import os\n",
    "import argparse\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import wandb\n",
    "from pathlib import Path\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KariRoadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, train=False):\n",
    "        self.root = Path(root)\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.img_dir = self.root/'train'/'images'\n",
    "        else:\n",
    "            self.img_dir = self.root/'val'/'images'\n",
    "        self.img_files = sorted(self.img_dir.glob('*.png'))\n",
    "        self.transform = get_transforms(train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file= self.img_files[idx].as_posix()\n",
    "        label_file = img_file.replace('images', 'labels')\n",
    "        img = cv2.imread(img_file)\n",
    "        label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n",
    "        img, label = self.transform(img, label)\n",
    "        return img, label, img_file\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "    \n",
    "class ImageAug:\n",
    "    def __init__(self, train):\n",
    "        if train:\n",
    "            self.aug = A.Compose([A.RandomCrop(256, 256),\n",
    "                                  A.HorizontalFlip(p=0.5),\n",
    "                                  A.ShiftScaleRotate(p=0.3),\n",
    "                                  A.RandomBrightnessContrast(p=0.3),\n",
    "                                  A.pytorch.transforms.ToTensorV2()])\n",
    "        else:\n",
    "            self.aug = ToTensorV2()\n",
    "\n",
    "    def __call__(self, img, label):\n",
    "        transformed = self.aug(image=img, mask=np.squeeze(label))\n",
    "        return transformed['image']/255.0, transformed['mask']\n",
    "\n",
    "def get_transforms(train):\n",
    "    transforms = ImageAug(train)\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "::: {.panel-tabset}\n",
    "\n",
    "### <span style=\"color:purple\">KariRoadDataset</span>\n",
    "\n",
    "::: {.panel-tabset}\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\"> __init__ </span>\n",
    "\n",
    "``` yaml\n",
    "def __init__(self, root, train=False):\n",
    "        self.root = Path(root)\n",
    "        self.train = train\n",
    "        if train:\n",
    "            self.img_dir = self.root/'train'/'images'\n",
    "        else:\n",
    "            self.img_dir = self.root/'val'/'images'\n",
    "        self.img_files = sorted(self.img_dir.glob('*.png'))\n",
    "        self.transform = get_transforms(train)\n",
    "```\n",
    "__init__ 메소드는 클래스가 초기화될 때<br> \n",
    " 학습 여부에 따라 데이터셋의 루트 경로와를 설정하고,  \n",
    "이 정보들을 클래스 내에서 사용할 수 있도록 준비합니다.\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">__getitem__</span>\n",
    "\n",
    "``` yaml\n",
    "def __getitem__(self, idx):\n",
    "    img_file= self.img_files[idx].as_posix()\n",
    "    label_file = img_file.replace('images', 'labels')\n",
    "    img = cv2.imread(img_file)\n",
    "    label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n",
    "    img, label = self.transform(img, label)\n",
    "    return img, label, img_file\n",
    "```\n",
    "### <span style=\"color:sky\">__len__</span>\n",
    "\n",
    "``` yaml\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "```\n",
    "\n",
    ":::\n",
    "### <span style=\"color:purple\">ImageAug</span>\n",
    "\n",
    "::: {.panel-tabset}\n",
    "\n",
    "### <span style=\"color:sky\">__init__</span>\n",
    "\n",
    "``` yaml\n",
    "def __init__(self, train):\n",
    "    if train:\n",
    "        self.aug = A.Compose([A.RandomCrop(256, 256),\n",
    "                                A.HorizontalFlip(p=0.5),\n",
    "                                A.ShiftScaleRotate(p=0.3),\n",
    "                                A.RandomBrightnessContrast(p=0.3),\n",
    "                                A.pytorch.transforms.ToTensorV2()])\n",
    "    else:\n",
    "        self.aug = ToTensorV2()\n",
    "```\n",
    "### <span style=\"color:sky\">__call__</span>\n",
    "\n",
    "``` yaml\n",
    "def __call__(self, img, label):\n",
    "    transformed = self.aug(image=img, mask=np.squeeze(label))\n",
    "    return transformed['image']/255.0, transformed['mask']\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    ":::\n",
    "### <span style=\"color:red\">get_transforms</span>\n",
    "\n",
    "``` yaml\n",
    "def get_transforms(train):\n",
    "    transforms = ImageAug(train)\n",
    "    return transforms\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_label(label):\n",
    "    h, w = label.shape\n",
    "    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n",
    "    colors = [\n",
    "        [0, 0, 0],          # 0: background 배경 (검은색)\n",
    "        [144, 124, 226],    # 1: motorway 고속도로 (보라색)\n",
    "        [172, 192, 251],    # 2: trunk 간선도로 (연한 파란색)\n",
    "        [161, 215, 253],    # 3: primary 주요도로 (연한 하늘색)\n",
    "        [187, 250, 246],    # 4: secondary 부차적인 도로 (연한 민트색)\n",
    "        [255, 255, 255],    # 5: tertiary 3차 도로 (흰색)\n",
    "        [49, 238, 75],      # 6: path 경로 (녹색)\n",
    "        [173, 173, 173],    # 7: under construction 건설 중 (회색)\n",
    "        [255, 85, 170],     # 8: train guideway 기차 안내로 (분홍색)\n",
    "        [234, 232, 120]     # 9: airplay runway 비행기 활주로 (노란색)\n",
    "    ]\n",
    "    for i in range(10):\n",
    "        color_label[label == i] = colors[i]\n",
    "    return color_label\n",
    "\n",
    "def plot_image(img, label=None, save_file='image.png', alpha=0.3):\n",
    "    # if img is tensor, convert to cv2 image\n",
    "    if torch.is_tensor(img):\n",
    "        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n",
    "\n",
    "    if label is not None:\n",
    "        # if label_img is tensor, convert to cv2 image\n",
    "        if torch.is_tensor(label):\n",
    "            label = label.cpu().numpy().astype(np.uint8)\n",
    "            color_label = make_color_label(label)\n",
    "            label = color_label\n",
    "        else:\n",
    "            color_label = make_color_label(label)\n",
    "            label = color_label\n",
    "        # overlay images\n",
    "        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n",
    "    # save image\n",
    "    cv2.imwrite(save_file, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "::: {.panel-tabset}\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\"> make_color_label </span>\n",
    "\n",
    "\n",
    "label의 데이터를 보면 1024*1024 사이즈의 검정사진이다.<br>\n",
    "2D 데이터라 눈으로 보이지 않지만 사진안에는 0~9의 데이터로 사진이 구성되어있다. 각 숫자에 맞게 색을 칠해 3D 컬러이미지로 변환하면 오른쪽과 같은 그림이 된다.\n",
    "**tip** 이 함수는 시각화를 위해 작성된 함수이고 학습에는 필수적인 함수는 아니다.\n",
    "\n",
    "\n",
    "라벨의 h(높이)와 w(너비)를 가져온뒤 h,w,3의 0으로된 넌파이 어레이를 만든다. <br>\n",
    "0부터 9 순서대로 색을 칠한다. 색 종류는 colors와 같이 칠한다.\n",
    "\n",
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "\n",
    "![검정색 label 파일](b_label.png)\n",
    "\n",
    ":::\n",
    "\n",
    "::: {.column width=\"1%\"}\n",
    ":::\n",
    "\n",
    "::: {.column width=\"49%\"}\n",
    "![색칠한 label 파일](c_label.png)\n",
    "\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">plot_image</span>\n",
    "\n",
    "\n",
    "1) 입력 이미지가 텐서 형식인 경우 numpy 배열로 변환.<br>\n",
    "2) 레이블이 주어진 경우 이를 컬러 이미지로 변환.<br>\n",
    "3) 원본 이미지와 레이블 이미지를 오버레이.<br>\n",
    "4) 최종 이미지를 파일로 저장.<br>\n",
    "\n",
    "![overlay된 이미지](ovlay.png)\n",
    "\n",
    "\n",
    "<사용된 핵심함수>\n",
    "``` python\n",
    "cv2.addWeighted(src1, alpha, src2, beta, gamma)\n",
    "```\n",
    "```\n",
    "src1: 첫 번째 입력 이미지.\n",
    "alpha: 첫 번째 이미지의 가중치.\n",
    "src2: 두 번째 입력 이미지.\n",
    "beta: 두 번째 이미지의 가중치.\n",
    "gamma: 결과 이미지에 더할 가중치 값\n",
    "```\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConfusionMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfusionMatrix:\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "        self.confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "        \n",
    "    def process_batch(self, preds, targets):\n",
    "        targets = targets.cpu().numpy().flatten()\n",
    "        preds = preds.argmax(1).cpu().numpy().flatten()\n",
    "        mask = (targets >= 0) & (targets < self.num_classes)\n",
    "        \n",
    "        confusion_mtx = np.bincount(\n",
    "                        self.num_classes * targets[mask].astype(int) + preds[mask],\n",
    "                        minlength=self.num_classes ** 2)\n",
    "        confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n",
    "        self.confusion_matrix += confusion_mtx\n",
    "        return confusion_mtx\n",
    "    \n",
    "    def print(self):\n",
    "        for i in range(self.num_classes):\n",
    "            print(f\"Class{i}:{self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n",
    "\n",
    "    def get_pix_acc(self):\n",
    "        return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
    "\n",
    "    def get_class_acc(self):\n",
    "        class_acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
    "        return np.nanmean(class_acc),class_acc\n",
    "    \n",
    "    def get_iou(self):\n",
    "        divisor = self.confusion_matrix.sum(axis=1)\\\n",
    "                    + self.confusion_matrix.sum(axis=0)\\\n",
    "                    - np.diag(self.confusion_matrix)\n",
    "        iou = np.diag(self.confusion_matrix) / divisor\n",
    "        return iou\n",
    "    \n",
    "    def get_mean_iou(self):\n",
    "        iou = self.get_iou()\n",
    "        return np.nansum(iou) / self.num_classes\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "\n",
    "::: {.panel-tabset}\n",
    "\n",
    "### <span style=\"color:purple\">ConfusionMatrix</span>\n",
    "\n",
    "::: {.panel-tabset}\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\"> __init__ </span>\n",
    "\n",
    "``` python\n",
    "def __init__(self, num_classes):\n",
    "    self.num_classes = num_classes\n",
    "    self.confusion_matrix = np.zeros((num_classes, num_classes))\n",
    "```\n",
    "__init__ 메소드는<br>\n",
    " 클래스가 초기화될 때 클래스 내에서 사용할 수 있도록 **준비물**을 준비하는 단계입니다.<br>\n",
    "```\n",
    "예시 상황\n",
    " class의 수를 4이라고 했을때, 0으로 구성된 4by4행렬이 준비된다.\n",
    "```\n",
    "\n",
    "![                   np.zeros((3,3))](033.png)    \n",
    "\n",
    "### <span style=\"color:sky\">process_batch</span>\n",
    "\n",
    "``` python\n",
    "def process_batch(self, preds, targets):\n",
    "    targets = targets.cpu().numpy().flatten()\n",
    "    preds = preds.argmax(1).cpu().numpy().flatten()\n",
    "    mask = (targets >= 0) & (targets < self.num_classes)\n",
    "    \n",
    "    confusion_mtx = np.bincount(\n",
    "                    self.num_classes * targets[mask].astype(int) + preds[mask],\n",
    "                    minlength=self.num_classes ** 2)\n",
    "    confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n",
    "    self.confusion_matrix += confusion_mtx\n",
    "```\n",
    "[설명] \n",
    "\n",
    "![이해를 돕기위해 사진의 targets과 preds을 process_batch에 적용해보려고 한다.](3by3.png)\n",
    "\n",
    "```\n",
    "mask는 사진에서 설정한 클래스(여기서는 4로 설정했으니깐 0~3) 값인 것만 남기고,\n",
    "그 외의 값은 제외하는 역할을 한다.\n",
    "```\n",
    "\n",
    "![정답traget.shape=(3,3) 추론값preds.shape=(3,3)을 flaten으로 일렬로 나열해 shape을 (9)로 만든다.](flaten.png)\n",
    "\n",
    "![[클래스*정답(9) + 추론값(9)]의 식을 이용해 인덱스를 계산한다. 결과=> [0, 6, 10, 13, 10, 5, 0, 15, 4] ](index_cal.png)\n",
    "\n",
    "![[0, 6, 10, 13, 10, 5, 0, 15, 4]에서 0은 2개 1은 0개 15은 1개 이런식으로 bincount로 원소값의 갯수를 샌다.<br>\n",
    "갯수의 결과가 [2, 0, 0, 0, 1, 1, 1, 0, 0, 0, 2, 0, 0, 1, 0, 1] 이렇게 나온다. <br>\n",
    "**tip.** minlength는 인덱스의 범위를 나타냄 이 식에서는 4x4 표를 만들것이기 때문에 4**2f로 설정](npbincount.png)\n",
    "\n",
    "![confusion_mtx.reshape(4, 4)를 통해 길이16벡터를 4by4 행렬로 변경 후, 방금 init에서 만들었던 0으로 구성된 4by4 self.confusion_matrix에 넣어준다.](행렬.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">print</span>\n",
    "\n",
    "``` python\n",
    "def print(self):\n",
    "    for i in range(self.num_classes):\n",
    "        print(f\"Class{i}:\n",
    "                {self.confusion_matrix[i,i]} / {self.confusion_matrix[i].sum()}\")\n",
    "```\n",
    "각 클래스별 accuracy를 표현 \n",
    "\n",
    "![](행렬.png)\n",
    "\n",
    "    Class0:2.0 / 2.0  <=정답0 2개 중 2개를 맞췄어\n",
    "    Class1:1.0 / 3.0  <=정답1 3개 중 1개를 맞췄어\n",
    "    Class2:2.0 / 2.0  <=정답1 2개 중 2개를 맞췄어\n",
    "    Class3:1.0 / 2.0  <=정답1 2개 중 1개를 맞췄어\n",
    "\n",
    "### <span style=\"color:sky\">get_pix_acc</span>\n",
    "\n",
    "``` python\n",
    "def get_pix_acc(self):\n",
    "    return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n",
    "```\n",
    "get_pix_acc 메서드는 픽셀 정확도(Pixel Accuracy)를 계산하는 역할<br>\n",
    "픽셀 정확도는 이미지 분할이나 분류 작업에서 예측된 픽셀이 실제 값과 일치하는 비율\n",
    "\n",
    "![(np.diag행렬의 가운데값을 모두 더한값(6)) 나누기 (confusion_matrix.sum행렬의 모든값 더한값(9))<br>\n",
    "get_pix_acc답 : 6/9    ](행렬.png)\n",
    "\n",
    "### <span style=\"color:sky\">get_class_acc</span>\n",
    "\n",
    "``` python\n",
    "def get_class_acc(self):\n",
    "    class_acc =np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n",
    "    return np.nanmean(class_acc),class_acc\n",
    "```\n",
    "\n",
    "도로 종류(클래스)가 비포장도로 국도 고속도로로 3개의 클래스가 있다고 하자\n",
    "도심에서 도로탐지를 한 뒤 평가하려고 하면 도시에 비포장도로가 없기에 accuracy를 구하면 \n",
    "비포장도로의 클래스는 0(Nan)값이 나오고 전체적 accuracy가 감소할 것이다. 이것을 방지하기\n",
    "위해서 get_class_acc을 쓰는것이다. 방법은 np.nanmean()을 쓰면 된다.\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">get_iou</span>\n",
    "\n",
    "``` python\n",
    "def get_iou(self):\n",
    "    divisor = self.confusion_matrix.sum(axis=1)\\\n",
    "                + self.confusion_matrix.sum(axis=0)\\\n",
    "                - np.diag(self.confusion_matrix)\n",
    "    iou = np.diag(self.confusion_matrix) / divisor\n",
    "    return iou\n",
    "```\n",
    "\n",
    "각 클래스별 iou를 구하는 코드이다. 출력값은 다음과 같이 나온다.<br>\n",
    "    >>IoU: [0.66666667 0.25 0.66666667 0.5 ]\n",
    "\n",
    ":::: {.columns}\n",
    "::: {.column width=\"45%\"}\n",
    "![iou 0.39 예시](iou1.png) \n",
    "    \n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "\n",
    "![iou는 Combined Region부분 나누기 Overlapping 이다. ](myiou.png)\n",
    ":::\n",
    "IOU는 예측한 픽셀과 실제 픽셀간 겹침 정도를 평가하는 지표이다. <br>\n",
    "사람이 눈으로 평가하는 지표와 가장 유사한 지표이다. <br>\n",
    "주로 객체 인식 알고리즘의 정확도를 평가하는데 사용된다.<br>\n",
    "::::\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">get_mean_iou</span>\n",
    "\n",
    "``` python\n",
    "    def get_mean_iou(self):\n",
    "        iou = self.get_iou()\n",
    "        return np.nansum(iou) / self.num_classes\n",
    "```\n",
    "\n",
    "get_iou의 평균값이다. <br>\n",
    "사진에 포함되지 않은 클래스가 있을 경우 iou값이 Nan값으로 나오게 된다. <br>\n",
    "Nan값을 제거하고 정확한 평균 계산을 위해 np.nansum()으로 더해준다.\n",
    "\n",
    ":::\n",
    "### <span style=\"color:purple\">사용예시</span>\n",
    "\n",
    "``` python\n",
    "ex_targets=torch.Tensor([[[0,1,2],[3,2,1],[0,3,1]]])#shape=(1,3,3)=(batchsize,H,W)\n",
    "ex_preds=torch.Tensor([[[[1,0,0], \n",
    "                     [0,0,0],\n",
    "                     [1,0,1]],\n",
    "                    [[0,0,0],\n",
    "                     [1,0,1],\n",
    "                     [0,0,0]],\n",
    "                    [[0,1,1],\n",
    "                     [0,1,0],\n",
    "                     [0,0,0]],\n",
    "                    [[0,0,0],\n",
    "                     [0,0,0],\n",
    "                     [0,1,0]]]]) #shape=(1,4,3,3)=(batchsize,C,H,W)\n",
    "\n",
    "\n",
    "# ConfusionMatrix 클래스 인스턴스화\n",
    "num_classes = 4  # 0 ~ 10 사이의 숫자를 사용하므로 총 11개의 클래스\n",
    "confusion_matrix = ConfusionMatrix(num_classes)\n",
    "\n",
    "# 배치 처리\n",
    "confusion_matrix.process_batch(ex_preds, ex_targets)\n",
    "\n",
    "# 메트릭 출력\n",
    "confusion_matrix.print()\n",
    "print(\"process_batch:\", confusion_matrix.process_batch(ex_preds, ex_targets))\n",
    "print(\"Pixel Accuracy:\", confusion_matrix.get_pix_acc())\n",
    "print(\"Class Accuracy:\", confusion_matrix.get_class_acc())\n",
    "print(\"IoU:\", confusion_matrix.get_iou())\n",
    "print(\"Mean IoU:\", confusion_matrix.get_mean_iou())\n",
    "```\n",
    ":::: {.columns}\n",
    "::: {.column width=\"60%\"}\n",
    "    결과\n",
    "    Class0:2.0 / 2.0  <=정답0 2개 중 2개를 맞췄어\n",
    "    Class1:1.0 / 3.0  <=정답1 3개 중 1개를 맞췄어\n",
    "    Class2:2.0 / 2.0  <=정답1 2개 중 2개를 맞췄어\n",
    "    Class3:1.0 / 2.0  <=정답1 2개 중 1개를 맞췄어\n",
    "    process_batch:\n",
    "    [[2 0 0 0]\n",
    "    [1 1 1 0]\n",
    "    [0 0 2 0]\n",
    "    [0 1 0 1]]\n",
    "    Pixel Accuracy: 0.6666\n",
    "    Class Accuracy: 0.7083\n",
    "    IoU: [0.66666667 0.25 0.66666667 0.5 ]\n",
    "    Mean IoU: 0.52083\n",
    "    \n",
    ":::\n",
    "\n",
    "::: {.column width=\"40%\"}\n",
    "\n",
    "![](3by3.png)\n",
    "\n",
    "\n",
    "\n",
    "![](행렬.png)\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "::::\n",
    "\n",
    "\n",
    "\n",
    ":::\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bce_loss(preds, targets, pos_weight=None):\n",
    "    bce_loss = F.binary_cross_entropy_with_logits(\n",
    "        preds.float(),\n",
    "        targets.float(),\n",
    "        pos_weight=pos_weight,\n",
    "    )\n",
    "    return bce_loss\n",
    "\n",
    "\n",
    "def ce_loss(preds, targets, ignore=255):\n",
    "    ce_loss = F.cross_entropy(\n",
    "        preds.float(),\n",
    "        targets.long(),    # [B, H, W]\n",
    "        ignore_index=ignore,\n",
    "    )\n",
    "    return ce_loss\n",
    "\n",
    "\n",
    "def dice_loss(preds, targets, eps=1e-7):\n",
    "    num_classes = preds.shape[1]\n",
    "    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)   # (B, 1, H, W) to (B, H, W, C)\n",
    "    true_1_hot = true_1_hot.permute(0, 3, 1, 2)                        # (B, H, W, C) to (B, C, H, W)\n",
    "    probas = F.softmax(preds, dim=1)\n",
    "    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n",
    "    dims = (0,) + tuple(range(2, targets.ndimension()))        # dims = (0, 2, 3)\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)     # intersection w.r.t. the class\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)      # cardinality w.r.t. the class\n",
    "    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n",
    "    return (1 - dice_loss)\n",
    "\n",
    "\n",
    "def jaccard_loss(preds, targets, eps=1e-7):\n",
    "    \"\"\"Computes the Jaccard loss.\n",
    "    Args:\n",
    "    preds(logits) a tensor of shape [B, C, H, W]\n",
    "    targets: a tensor of shape [B, 1, H, W].\n",
    "    eps: added to the denominator for numerical stability.\n",
    "    Returns:\n",
    "        Jaccard loss\n",
    "    \"\"\"\n",
    "    num_classes = preds.shape[1]\n",
    "    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n",
    "    true_1_hot = true_1_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n",
    "    probas = F.softmax(preds, dim=1)\n",
    "    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n",
    "    dims = (0,) + tuple(range(2, targets.ndimension()))\n",
    "    intersection = torch.sum(probas * true_1_hot, dims)\n",
    "    cardinality = torch.sum(probas + true_1_hot, dims)\n",
    "    union = cardinality - intersection\n",
    "    jacc_loss = (intersection / (union + eps)).mean()\n",
    "    return (1 - jacc_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.panel-tabset}\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\"> bce_loss </span>\n",
    "\n",
    "``` yaml\n",
    "가중치가 적용된 이진 교차 엔트로피 손실을 계산\n",
    "Args:\n",
    "    targets: [B, 1, H, W] 형태의 텐서. 실제 값 (레이블).\n",
    "    preds: [B, 1, H, W] 형태의 텐서. 모델의 예측 값.\n",
    "    pos_weight: 양성 클래스에 대한 가중치 (선택 사항).\n",
    "\n",
    "Returns:\n",
    "    bce_loss: 가중치가 적용된 이진 교차 엔트로피 손실.\n",
    "```\n",
    "\n",
    "![BCE LOSS](BCE.png)\n",
    "\n",
    "* N: 데이터의 수\n",
    "* y𝑖𝑐 : 실제 레이블 (0 또는 1)\n",
    "* p𝑖𝑐 : 모델이 예측한 클래스 1에 속할 확률 \n",
    "\n",
    "목적: BCE의 주된 목적은 이진 분류 모델의 예측값 𝑝𝑖가 실제 레이블 𝑦𝑖와 얼마나 잘 일치하는지를 측정하는 것입니다. 이 손실 함수는 예측값과 실제 레이블 간의 차이를 최소화하도록 모델을 학습시킵니다.\n",
    "\n",
    "손실 계산: BCE는 각 데이터 포인트에 대해 예측값 𝑝𝑖가 실제 레이블 𝑦𝑖와 얼마나 비슷한지를 측정합니다. 예측값이 실제 레이블과 정확히 일치할 때 손실은 0이 되고, 그렇지 않으면 손실이 증가합니다. 이를 통해 모델이 더 정확한 예측을 하도록 유도합니다.\n",
    "\n",
    "확률적 해석: BCE는 각 예측값 𝑝𝑖을 클래스 1에 속할 확률로 해석하며, 이를 로그 함수를 통해 손실을 계산합니다. 따라서 모델은 클래스 1에 속할 확률을 정확히 예측하도록 학습됩니다.\n",
    "\n",
    "최적화: BCE는 일반적으로 이진 분류 모델의 손실 함수로 사용되며, 경사 하강법을 통해 모델의 가중치를 조정하여 손실을 최소화하도록 합니다. 이를 통해 모델이 데이터의 패턴을 학습하고, 새로운 데이터에 대해 정확한 예측을 할 수 있도록 합니다.\n",
    "\n",
    "### <span style=\"color:sky\">ce_loss</span>\n",
    "\n",
    "``` yaml\n",
    "가중치가 적용된 다중 클래스 교차 엔트로피 손실을 계산\n",
    "\n",
    "Args:\n",
    "    targets: [B, H, W] 형태의 텐서. 실제 값 (레이블).\n",
    "    preds: [B, C, H, W] 형태의 텐서. 모델의 예측 값.\n",
    "    ignore: 무시할 클래스 인덱스.\n",
    "\n",
    "Returns:\n",
    "    ce_loss: 가중치가 적용된 다중 클래스 교차 엔트로피 손실.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "![CE LOSS](CE.png)\n",
    "\n",
    "* N: 데이터의 수\n",
    "* C: 클래스의 수\n",
    "* y𝑖𝑐 : 데이터 포인트 i의 실제 클래스 𝑐에 대한 one-hot 인코딩된 레이블 (0 또는 1)\n",
    "* p𝑖𝑐 : 모델이 예측한 데이터 포인트 𝑖가 클래스 c에 속할 확률 \n",
    "\n",
    "목적: CE 손실 함수는 다중 클래스 분류 모델에서 예측값 pic가 실제 레이블yic와 얼마나 일치하는지를 측정합니다. 모델이 다양한 클래스를 분류하고, 각 클래스에 속할 확률을 정확하게 예측하도록 학습시키는 데 사용됩니다.\n",
    "\n",
    "손실 계산: 각 데이터 포인트에 대해 CE는 각 클래스에 대해 예측된 확률 pic가 실제 레이블 yic 사이의 교차 엔트로피를 계산하여 전체 데이터셋에 대한 평균 손실을 계산합니다. 이 손실은 모델이 예측을 향상시키고, 다양한 클래스 간의 결정 경계를 명확하게 만들도록 돕습니다.\n",
    "\n",
    "확률적 해석: CE 손실 함수는 예측된 확률 pic을 클래스 c에 속할 확률로 해석하며, 이를 로그 함수를 통해 손실을 계산합니다. 따라서 모델은 각 데이터 포인트가 각 클래스에 속할 가능성을 정확히 예측하도록 학습됩니다.\n",
    "\n",
    "최적화: CE 손실 함수는 다중 클래스 분류 문제에서 모델을 훈련시키는 데 주로 사용됩니다. 경사 하강법을 통해 모델의 가중치를 조정하여 CE 손실을 최소화하도록 하여 모델의 성능을 향상시킵니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">dice_loss</span>\n",
    "\n",
    "``` yaml\n",
    "Sørensen–Dice 손실을 계산합니다.\n",
    "\n",
    "Args:\n",
    "    preds(logits): [B, C, H, W] 형태의 텐서. 모델의 예측 값 (로짓).\n",
    "    targets: [B, 1, H, W] 형태의 텐서. 실제 값 (레이블).\n",
    "    eps: 분모에 더해지는 작은 값으로, 수치적 안정성을 위해 사용됩니다.\n",
    "\n",
    "Returns:\n",
    "    dice_loss: Sørensen–Dice 손실 값.\n",
    "```\n",
    "\n",
    "![DICE LOSS](DICE1.png)\n",
    "\n",
    "* ∣P∩T∣: 예측된 세그멘테이션 영역 P과 실제 타겟 세그멘테이션 영역 T의 교집합의 크기입니다.\n",
    "\n",
    "* ∣P∣: 예측된 세그멘테이션 영역 P의 크기 또는 원소 수입니다.\n",
    "\n",
    "* ∣T∣: 실제 타겟 세그멘테이션 영역 T의 크기 또는 원소 수입니다.\n",
    "\n",
    "목적: Dice 계수는 두 세그멘테이션 영역의 중첩 정도를 측정하여 예측된 세그멘테이션 𝑃이 실제 타겟 세그멘테이션 T과 얼마나 일치하는지를 평가합니다. 이 지표는 0에서 1 사이의 값을 가지며, 1에 가까울수록 두 영역이 완전히 일치함을 나타냅니다.\n",
    "\n",
    "계산 방법: Dice 계수의 분자 2×∣𝑃∩𝑇∣는 예측된 영역과 실제 타겟 영역의 교집합의 크기를 두 배로 확장하여, under-segmentation과 over-segmentation을 모두 공평하게 패널티를 줍니다. 분모 ∣P∣+∣T∣는 두 영역의 크기 합으로 정규화하여 계수가 [0, 1] 범위 내에 유지되도록 합니다.\n",
    "\n",
    "응용: Dice 계수는 주로 의료 이미지 분석 및 다양한 영상 처리 애플리케이션에서 사용됩니다. 이미지 세분화 알고리즘의 정확도를 평가하고, 예측된 세그멘테이션의 질을 비교하는 데 중요한 지표 역할을 합니다.\n",
    "\n",
    "### <span style=\"color:sky\">jaccard_loss</span>\n",
    "\n",
    "``` yaml\n",
    "    Jaccard 손실을 계산합니다.\n",
    "    인수:\n",
    "    preds(logits): 형상이 [B, C, H, W]인 텐서\n",
    "    targets: 형상이 [B, 1, H, W]인 텐서\n",
    "    eps: 수치 안정성을 위해 분모에 추가되는 작은 값\n",
    "    반환:\n",
    "    Jaccard 손실\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "![JACCARD LOSS](JACCARD.png)\n",
    "\n",
    "* ∣P∩T∣: 예측된 영역 𝑃과 실제 타겟 영역 𝑇의 교집합의 크기\n",
    "* ∣P∪T∣: 예측된 영역 𝑃과 실제 타겟 영역 𝑇의 합집합의 크기\n",
    "\n",
    "목적: Jaccard 지수는 두 세그멘테이션 영역의 중첩 정도를 측정하여 예측된 세그멘테이 P이 실제 타겟 세그멘테이션 T과 얼마나 일치하는지를 평가합니다. 이 지표는 0에서 1 사이의 값을 가지며, 1에 가까울수록 두 영역이 완전히 일치함을 나타냅니다.\n",
    "\n",
    "계산 방법: Jaccard 지수는 예측된 영역 𝑃과 실제 타겟 영역 𝑇의 교집합 크기를 두 영역의 합집합 크기로 나누어 계산합니다. 이는 두 영역이 얼마나 겹치는지를 평가하며, 분자는 교집합의 크기를 나타내고 분모는 전체적인 예측된 영역과 실제 타겟 영역의 크기를 반영합니다.\n",
    "\n",
    "응용: Jaccard 지수는 주로 이미지 세분화의 성능을 평가하는 데 사용됩니다. 다양한 이미지 처리 및 패턴 인식 문제에서 유사성을 비교하고 모델의 정확도를 평가하는 데 중요한 지표로 활용됩니다.\n",
    ":::\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_epoch( train, val )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(train_dataloader, model, optimizer, device):\n",
    "    model.train()\n",
    "    losses = [] \n",
    "    for i, (imgs, targets, _) in enumerate(train_dataloader):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        preds = model(imgs)['out']     # forward \n",
    "        loss = ce_loss(preds, targets) # calculates the iteration loss  \n",
    "        optimizer.zero_grad()   # zeros the parameter gradients\n",
    "        loss.backward()         # backward\n",
    "        optimizer.step()        # update weights\n",
    "        print('\\t iteration: %d/%d, loss=%.4f' % (i, len(train_dataloader)-1, loss))    \n",
    "        losses.append(loss.item())\n",
    "    return torch.tensor(losses).mean().item()\n",
    "\n",
    "\n",
    "def val_one_epoch(val_dataloader, model, confusion_matrix, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i, (imgs, targets, img_file) in enumerate(val_dataloader):\n",
    "        imgs, targets = imgs.to(device), targets.to(device)\n",
    "        with torch.no_grad():\n",
    "            preds = model(imgs)['out']   # forward, preds: (B, 2, H, W)\n",
    "            loss = ce_loss(preds, targets)\n",
    "            losses.append(loss.item())\n",
    "            confusion_matrix.process_batch(preds, targets)\n",
    "            # sample images\n",
    "            if i == 0:\n",
    "                preds = torch.argmax(preds, axis=1) # (1, H, W)  \n",
    "                for j in range(3):\n",
    "                    save_file = os.path.join('outputs', 'val_%d.png' % (j))\n",
    "                    plot_image(imgs[j], preds[j], save_file)\n",
    "                \n",
    "    avg_loss = torch.tensor(losses).mean().item()\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.panel-tabset}\n",
    "\n",
    "### <span style=\"color:sky\">train_one_epoch</span>\n",
    "```\n",
    "1) def train_one_epoch(train_dataloader, model, optimizer, device)이라는\n",
    "    함수는 파라미터가 4개이다.\n",
    "2) model.train()은 학습모드인것을 나타낸다.\n",
    "3) losses = []는 train의 loss값을 저장하기 위함이다.\n",
    "4) 반복문(for)의 i 는 enumerate함수에서 만들어진(인덱싱된) 0부터 n까지의\n",
    "    숫자이고 (imgs, targets, _)는 각각 imgs는 위성사진 targets는 정답라벨이다.\n",
    "5) device를 통해 cpu에서 gpu로 이동시키는 과정으로 device는 뒤에서 정의할 것이다.\n",
    "6) model에 img를 집어넣어 preds를 예측해본다. 이때 [out]이 붙는 이유는\n",
    "    torchvision에 저장되어 있는 deeplabv3모델을 사용할때의 방식이다.\n",
    "7) loss는 앞에서 정의해준 로스 중 ce_loss를 사용했다. \n",
    "    이때 preds가 먼저 들어가야하는 점을 주의하자\n",
    "8) optimizer.zero는 loss의 미분값을 계산하기전 0으로 초기화하는 과정이다.\n",
    "9) loss.backward()는 loss값을 편미분하는 과정이다. \n",
    "    편미분은 2차원 공간에서 미분을 한 번에 못하니깐 \n",
    "    순차적으로 미분하는 과정이라고 생각하면 된다.\n",
    "10) 미분한 값을 모델의 파라미터값에 반영한다(미분값을 빼주는과정). \n",
    "11) iteration별 train loss를 표시해준다.\n",
    "12) loss를 losses에 저장한다.\n",
    "13) return을 통해 함수 밖에서도 losses의 평균을 호출 할 수 있게한다.\n",
    "```\n",
    "\n",
    "### <span style=\"color:sky\">val_one_epoch</span>\n",
    "```\n",
    "1) def val_one_epoch(val_dataloader, model, confusion_matrix, device)이라는\n",
    "    함수는 파라미터가 4개이다.\n",
    "2) model.eval()은 학습모드인것을 나타낸다.\n",
    "3) losses = []는 validation의 loss값을 저장하기 위함이다.\n",
    "4) 반복문(for)의 i 는 enumerate함수에서 만들어진(인덱싱된) 0부터 n까지의\n",
    "    숫자이고 (imgs, targets, _)는 각각 imgs는 위성사진 targets는 정답라벨이다.\n",
    "5) device를 통해 cpu에서 gpu로 이동시키는 과정으로 device는 뒤에서 정의할 것이다.\n",
    "6) model에 img를 집어넣어 preds를 예측해본다. 이때 [out]이 붙는 이유는\n",
    "    torchvision에 저장되어 있는 deeplabv3모델을 사용할때의 방식이다.\n",
    "7) loss는 앞에서 정의해준 로스 중 ce_loss를 사용했다. \n",
    "    이때 preds가 먼저 들어가야하는 점을 주의하자\n",
    "8) loss를 losses에 저장한다.\n",
    "9) 이후에 ConfusionMatrix를 이용해 confusion_matrix라는 인스턴스를 만들게\n",
    " 되는데 만들어진 인스턴스에 모델이 예측한 preds와 정답지 targets을 넣어주면 \n",
    " 행렬이 계산이 돼서 아래와 같이 사용가능하다.\n",
    "``` \n",
    "            val_epoch_iou = confusion_matrix.get_iou()\n",
    "            val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n",
    "            val_epoch_pix_accuracy = confusion_matrix.get_pix_acc() \n",
    "```\n",
    "\n",
    "12) ~ 16) i=0일때(epoch이 새로 시작될때)마다 plot_image 함수를 이용해 \n",
    "img에 preds를 오버레이해서 사진으로 저장한다.(총 3장 저장한다.) \n",
    "추가로 저장된 사진은 이름이 val_0(1,2).png으로 매 에폭마다 사진이 바뀐다. \n",
    "따로 저장하려면 수정필요\n",
    "17~) return을 통해 함수 밖에서도 validation losses의 평균을 호출 할 수 있게한다.\n",
    "```\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train______________________________________________________#   \n",
    "def train(epochs=200, batch_size = 8 , name = 'suwany'):\n",
    "#1______________________________________________________________#    \n",
    "    # wandb settings\n",
    "    wandb.init(id=name, resume='allow', mode='disabled')\n",
    "    wandb.config.update({\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'name': name\n",
    "    })\n",
    "#2______________________________________________________________#   \n",
    "    # Train dataset\n",
    "    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n",
    "    # Train dataloader\n",
    "    num_workers = min([os.cpu_count(), batch_size, 16])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # Validation dataset\n",
    "    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "#3______________________________________________________________#   \n",
    "    # Network model\n",
    "    num_classes = 10 # background + 1 classes\n",
    "    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n",
    "    \n",
    "    # GPU-support\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.device_count() > 1:   # multi-GPU\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "#______________________________________________________________#   \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "      \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "#______________________________________________________________#   \n",
    "    # loading a weight file (if exists)\n",
    "    weight_file = Path('weights')/(name + '.pth')\n",
    "    best_accuracy = 0.0\n",
    "    start_epoch, end_epoch = (0, epochs)\n",
    "    if os.path.exists(weight_file):\n",
    "        checkpoint = torch.load(weight_file)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_accuracy = checkpoint['best_accuracy']\n",
    "        print('resumed from epoch %d' % start_epoch)\n",
    "#______________________________________________________________#       \n",
    "    \n",
    "    confusion_matrix = ConfusionMatrix(num_classes)\n",
    " # training/validation\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        print('epoch: %d/%d' % (epoch, end_epoch-1))\n",
    "        t0 = time.time()\n",
    "        # training\n",
    "        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n",
    "        t1 = time.time()\n",
    "        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        # validation\n",
    "        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n",
    "        val_epoch_iou = confusion_matrix.get_iou()\n",
    "        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n",
    "        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n",
    "        \n",
    "        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n",
    "              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n",
    "        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n",
    "        # saving the best status into a weight file\n",
    "        if val_epoch_pix_accuracy > best_accuracy:\n",
    "             best_weight_file = Path('weights')/(name + '_best.pth')\n",
    "             best_accuracy = val_epoch_pix_accuracy\n",
    "             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n",
    "             torch.save(state, best_weight_file)\n",
    "             print('best accuracy=>saved\\n')\n",
    "        # saving the current status into a weight file\n",
    "        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n",
    "        torch.save(state, weight_file)\n",
    "        # wandb logging\n",
    "        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n",
    "#______________________________________________________________#          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.panel-tabset}\n",
    "\n",
    "### <span style=\"color:sky\">함수 train 소개</span>\n",
    "\n",
    "이 함수는 지금까지 정의한 함수들을 이용함, 주어진 하이퍼파라미터 설정을 기반으로 모델을 학습하고 검증하는 과정을 포함되어있다. 또한, 모델의 상태를 저장하고, WandB(Weights and Biases)를 사용하여 로그를 기록한다.\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">def train</span>\n",
    "```\n",
    "def train(epochs=200, batch_size = 8 , name = 'suwany'):\n",
    "\n",
    "```\n",
    "* epochs: 학습할 총 에포크 수입니다. 기본값은 200입니다.\n",
    "* batch_size: 미니배치의 크기입니다. 기본값은 8입니다.\n",
    "* name: 실험 이름입니다. 기본값은 'suwany'입니다.\n",
    "\n",
    "\n",
    "### <span style=\"color:sky\">wandb</span>\n",
    "```\n",
    "    wandb.init(id=name, resume='allow', mode='disabled')\n",
    "    wandb.config.update({\n",
    "        'epochs': epochs,\n",
    "        'batch_size': batch_size,\n",
    "        'name': name\n",
    "    })\n",
    "```\n",
    "* WandB를 초기화하고 설정을 업데이트합니다. 이로 인해 실험의 하이퍼파라미터가 기록됩니다\n",
    "\n",
    "### <span style=\"color:sky\">dataset, dataloader</span>\n",
    "```\n",
    "    # Train dataset\n",
    "    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n",
    "    # Train dataloader\n",
    "    num_workers = min([os.cpu_count(), batch_size, 16])\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "    # Validation dataset\n",
    "    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n",
    "                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n",
    "\n",
    "```\n",
    "* KariRoadDataset 클래스를 사용하여 학습 및 검증 데이터셋을 초기화합니다.\n",
    "* 각 데이터셋에 대해 데이터로더를 생성하여 데이터를 배치 단위로 로드합니다.\n",
    "* num_workers는 데이터 로딩을 위한 CPU 스레드 수를 설정합니다.\n",
    "\n",
    "### <span style=\"color:sky\">num_classes, model, device  </span>\n",
    "```\n",
    "    num_classes = 10  # background + 9 classes\n",
    "    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n",
    "    \n",
    "    # GPU-support\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.device_count() > 1:   # multi-GPU\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "```\n",
    "* num_classes를 설정하여 모델의 클래스 수를 정의한다.\n",
    "    + 여기서는 road의 클래스 9개에 배경 1개를 추가한다.\n",
    "* DeepLabV3 모델을 ResNet-101 백본을 사용하여 초기화했다.\n",
    "* 모델을 GPU로 이동시키며, 여러 GPU를 사용할 경우 DataParallel을 사용하도록 했다.\n",
    "\n",
    "### <span style=\"color:sky\">Optimizer,lr_scheduler </span>\n",
    "```\n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
    "      \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "```\n",
    "* Adam 옵티마이저를 설정하여 모델 파라미터를 최적화한다.\n",
    "* 학습률 스케줄러는 검증 손실이 5회 이상 줄어들지 않을 때 학습률을 감소시키도록 설정했다.\n",
    "\n",
    "### <span style=\"color:sky\">load</span>\n",
    "```\n",
    "    weight_file = Path('weights')/(name + '.pth')\n",
    "    best_accuracy = 0.0\n",
    "    start_epoch, end_epoch = (0, epochs)\n",
    "    if os.path.exists(weight_file):\n",
    "        checkpoint = torch.load(weight_file)\n",
    "        model.load_state_dict(checkpoint['model'])\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        best_accuracy = checkpoint['best_accuracy']\n",
    "        print('resumed from epoch %d' % start_epoch)\n",
    "\n",
    "```\n",
    "기존에 저장된 체크포인트 파일이 있으면 이를 로드하여 학습을 이어서 진행할 수 있다.\n",
    "\n",
    "### <span style=\"color:sky\">train_start</span>\n",
    "```\n",
    "    confusion_matrix = ConfusionMatrix(num_classes)\n",
    "    # training/validation\n",
    "    for epoch in range(start_epoch, end_epoch):\n",
    "        print('epoch: %d/%d' % (epoch, end_epoch-1))\n",
    "        t0 = time.time()\n",
    "        # training\n",
    "        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n",
    "        t1 = time.time()\n",
    "        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n",
    "        lr_scheduler.step(epoch_loss)\n",
    "        # validation\n",
    "        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n",
    "        val_epoch_iou = confusion_matrix.get_iou()\n",
    "        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n",
    "        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n",
    "        \n",
    "        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n",
    "              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n",
    "        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n",
    "        # saving the best status into a weight file\n",
    "        if val_epoch_pix_accuracy > best_accuracy:\n",
    "             best_weight_file = Path('weights')/(name + '_best.pth')\n",
    "             best_accuracy = val_epoch_pix_accuracy\n",
    "             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n",
    "             torch.save(state, best_weight_file)\n",
    "             print('best accuracy=>saved\\n')\n",
    "        # saving the current status into a weight file\n",
    "        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n",
    "        torch.save(state, weight_file)\n",
    "        # wandb logging\n",
    "        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n",
    "\n",
    "```\n",
    "* for문을 이용해 각 에포크마다 학습 및 검증을 수행한다.\n",
    "* train_one_epoch 함수는 한 에포크 동안 학습을 수행하며, 손실 값을 반환했다.\n",
    "* 검증 단계에서는 val_one_epoch 함수를 통해 검증 손실, IoU, 평균 IoU, 픽셀 정확도를 계산한다.\n",
    "* 최상의 모델 상태를 저장하며, 현재 상태도 에폭마다 저장되도록 했다.\n",
    "* WandB를 사용하여 학습 및 검증 손실, 정확도를 로깅되어 기록된다.\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실행 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0/199\n",
      "\t iteration: 0/142, loss=2.2205\n",
      "\t iteration: 1/142, loss=2.1528\n",
      "\t iteration: 2/142, loss=2.1019\n",
      "\t iteration: 3/142, loss=2.0039\n",
      "\t iteration: 4/142, loss=1.9516\n",
      "\t iteration: 5/142, loss=1.8579\n",
      "\t iteration: 6/142, loss=1.8787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuwany\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 58\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m t0 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# training\u001b[39;00m\n\u001b[0;32m---> 58\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m t1 \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss=\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m (took \u001b[39m\u001b[38;5;132;01m%.2f\u001b[39;00m\u001b[38;5;124m sec)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (epoch_loss, t1\u001b[38;5;241m-\u001b[39mt0))\n",
      "Cell \u001b[0;32mIn[14], line 9\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(train_dataloader, model, optimizer, device)\u001b[0m\n\u001b[1;32m      7\u001b[0m loss \u001b[38;5;241m=\u001b[39m ce_loss(preds, targets) \u001b[38;5;66;03m# calculates the iteration loss  \u001b[39;00m\n\u001b[1;32m      8\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m# zeros the parameter gradients\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m# backward\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()        \u001b[38;5;66;03m# update weights\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m iteration: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, loss=\u001b[39m\u001b[38;5;132;01m%.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (i, \u001b[38;5;28mlen\u001b[39m(train_dataloader)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, loss))    \n",
      "File \u001b[0;32m~/anaconda3/envs/gd/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/gd/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(epochs=200, batch_size = 8 , name = 'suwany')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::: {.columns}\n",
    "\n",
    "::: {.column width=\"50%\"}\n",
    "```python{.bash filename=\"학교\"}\n",
    "---\n",
    "전북대학교 지구환경과학과\n",
    "---\n",
    "```\n",
    ":::\n",
    "\n",
    "::: {.column width=\"1%\"}\n",
    ":::\n",
    "\n",
    "::: {.column width=\"49%\"}\n",
    "``` {.bash filename=\"이름\"}\n",
    "---\n",
    "김수환\n",
    "---\n",
    "```\n",
    ":::\n",
    "\n",
    "::::"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
