[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/road_seg.html",
    "href": "posts/post-with-code/road_seg.html",
    "title": "Road_segmantation",
    "section": "",
    "text": "This is a post with executable code.\n\nImports\n\n#Datasets\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n#Utils\nimport os\nimport cv2\nimport numpy as np\nimport torch\n\n#ConfusionMatrix\nimport numpy as np\n\n#Loss\nimport torch\nimport torch.nn.functional as F\n\n#Train\nimport os\nimport argparse\nimport torch\nimport torch.optim as optim\nimport time\nimport wandb\nfrom pathlib import Path\nfrom torchvision import models\n\n\n\nDataset\n\nclass KariRoadDataset(torch.utils.data.Dataset):\n    def __init__(self, root, train=False):\n        self.root = Path(root)\n        self.train = train\n        if train:\n            self.img_dir = self.root/'train'/'images'\n        else:\n            self.img_dir = self.root/'val'/'images'\n        self.img_files = sorted(self.img_dir.glob('*.png'))\n        self.transform = get_transforms(train)\n\n    def __getitem__(self, idx):\n        img_file= self.img_files[idx].as_posix()\n        label_file = img_file.replace('images', 'labels')\n        img = cv2.imread(img_file)\n        label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)\n        img, label = self.transform(img, label)\n        return img, label, img_file\n\n    def __len__(self):\n        return len(self.img_files)\n    \nclass ImageAug:\n    def __init__(self, train):\n        if train:\n            self.aug = A.Compose([A.RandomCrop(256, 256),\n                                  A.HorizontalFlip(p=0.5),\n                                  A.ShiftScaleRotate(p=0.3),\n                                  A.RandomBrightnessContrast(p=0.3),\n                                  A.pytorch.transforms.ToTensorV2()])\n        else:\n            self.aug = ToTensorV2()\n\n    def __call__(self, img, label):\n        transformed = self.aug(image=img, mask=np.squeeze(label))\n        return transformed['image']/255.0, transformed['mask']\n\ndef get_transforms(train):\n    transforms = ImageAug(train)\n    return transforms\n\n\n\nUtils\n\ndef plot_img_file(img_file):\n    img = cv2.imread(img_file)\n    output_file = os.path.join('outputs', os.path.basename(img_file))\n    cv2.imwrite(output_file, img)\n\n\ndef make_color_label_file(label_file):\n    label = cv2.imread(label_file, cv2.IMREAD_GRAYSCALE)  # (H, W) shape\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\n\n\ndef make_color_label(label):\n    h, w = label.shape\n    color_label = np.zeros((h, w, 3), dtype=np.uint8)  # (H, W, 3) shape\n    colors = [\n        [0, 0, 0],          # 0: background\n        [144, 124, 226],    # 1: motorway\n        [172, 192, 251],    # 2: trunk\n        [161, 215, 253],    # 3: primary\n        [187, 250, 246],    # 4: secondary\n        [255, 255, 255],    # 5: tertiary\n        [49, 238, 75],      # 6: path\n        [173, 173, 173],    # 7: under construction\n        [255, 85, 170],     # 8: train guideway\n        [234, 232, 120]     # 9: airplay runway\n    ]\n    for i in range(10):\n        color_label[label == i] = colors[i]\n    return color_label\n\n\ndef plot_image_label_file(img_file, alpha=0.3):\n    img = cv2.imread(img_file)\n    label_file = img_file.replace('images', 'labels')\n    color_label = make_color_label_file(label_file)\n    overlay = cv2.addWeighted(img, 1, color_label, alpha, 0)\n    output_file = os.path.join('outputs', os.path.basename(img_file).replace('.png', '_overlay.png'))\n    cv2.imwrite(output_file, overlay)\n\n\ndef plot_image(img, label=None, save_file='image.png', alpha=0.3):\n    # if img is tensor, convert to cv2 image\n    if torch.is_tensor(img):\n        img = img.mul(255.0).cpu().numpy().transpose(1, 2, 0).astype(np.uint8)\n\n    if label is not None:\n        # if label_img is tensor, convert to cv2 image\n        if torch.is_tensor(label):\n            label = label.cpu().numpy().astype(np.uint8)\n            color_label = make_color_label(label)\n            label = color_label\n        else:\n            color_label = make_color_label(label)\n            label = color_label\n        # overlay images\n        img = cv2.addWeighted(img, 1.0, label, alpha, 0)\n    # save image\n    cv2.imwrite(save_file, img)\n\n\n\nConfusionMatrix\n\nclass ConfusionMatrix:\n    def __init__(self, num_classes):\n        self.num_classes = num_classes\n        self.confusion_matrix = np.zeros((num_classes, num_classes))\n        # self.confusion_matrix_torch = torch.zeros((num_classes, num_classes))\n\n    def process_batch(self, preds, targets):\n        preds = preds.argmax(1).cpu().numpy().flatten()\n        targets = targets.cpu().numpy().flatten()\n        mask = (targets &gt;= 0) & (targets &lt; self.num_classes)\n        \n        confusion_mtx = np.bincount(self.num_classes * targets[mask].astype(int) + preds[mask],\n                        minlength=self.num_classes ** 2)\n        confusion_mtx = confusion_mtx.reshape(self.num_classes, self.num_classes)\n        self.confusion_matrix += confusion_mtx\n\n    def print(self):\n        for i in range(self.num_classes):\n            print(f\"Class {i}: {self.confusion_matrix[i, i]} / {self.confusion_matrix[i].sum()}\")\n\n    def get_pix_acc(self):\n        return np.diag(self.confusion_matrix).sum() / self.confusion_matrix.sum()\n\n    def get_class_acc(self):\n        class_acc = np.diag(self.confusion_matrix) / self.confusion_matrix.sum(axis=1)\n        return np.nanmean(class_acc)\n    \n    def get_iou(self):\n        divisor = self.confusion_matrix.sum(axis=1) + self.confusion_matrix.sum(axis=0) - \\\n                    np.diag(self.confusion_matrix)\n        iou = np.diag(self.confusion_matrix) / divisor\n        return iou\n    \n    def get_mean_iou(self):\n        iou = self.get_iou()\n        return np.nansum(iou) / self.num_classes\n    \n\n\n\nLoss\n\ndef bce_loss(preds, targets, pos_weight=None):\n    \"\"\"Computes the weighted binary cross-entropy loss.\n    Args:\n        targets: a tensor of shape [B, 1, H, W].\n        preds: a tensor of shape [B, 1, H, W]\n    Returns:\n        bce_loss: the weighted binary cross-entropy loss.\n    \"\"\"\n    bce_loss = F.binary_cross_entropy_with_logits(\n        preds.float(),\n        targets.float(),\n        pos_weight=pos_weight,\n    )\n    return bce_loss\n\n\ndef ce_loss(preds, targets, ignore=255):\n    \"\"\"Computes the weighted multi-class cross-entropy loss.\n    Args:\n        targets: a tensor of shape [B, H, W].\n        preds: a tensor of shape [B, C, H, W]. \n        ignore: the class index to ignore.\n    Returns:\n        ce_loss: the weighted multi-class cross-entropy loss.\n    \"\"\"\n    ce_loss = F.cross_entropy(\n        preds.float(),\n        targets.long(),    # [B, H, W]\n        ignore_index=ignore,\n    )\n    return ce_loss\n\n\ndef dice_loss(preds, targets, eps=1e-7):\n    \"\"\"Computes the Sørensen–Dice loss.\n    Args:\n    preds(logits) a tensor of shape [B, C, H, W]\n    targets: a tensor of shape [B, 1, H, W].\n    eps: added to the denominator for numerical stability.\n    Returns:\n        dice_loss: the Sørensen–Dice loss.\n    \"\"\"\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)   # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)                        # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))        # dims = (0, 2, 3)\n    intersection = torch.sum(probas * true_1_hot, dims)     # intersection w.r.t. the class\n    cardinality = torch.sum(probas + true_1_hot, dims)      # cardinality w.r.t. the class\n    dice_loss = (2. * intersection / (cardinality + eps)).mean()\n    return (1 - dice_loss)\n\n\ndef jaccard_loss(preds, targets, eps=1e-7):\n    \"\"\"Computes the Jaccard loss.\n    Args:\n    preds(logits) a tensor of shape [B, C, H, W]\n    targets: a tensor of shape [B, 1, H, W].\n    eps: added to the denominator for numerical stability.\n    Returns:\n        Jaccard loss\n    \"\"\"\n    num_classes = preds.shape[1]\n    true_1_hot = F.one_hot(targets.squeeze(1), num_classes=num_classes)  # (B, 1, H, W) to (B, H, W, C)\n    true_1_hot = true_1_hot.permute(0, 3, 1, 2)  # (B, H, W, C) to (B, C, H, W)\n    probas = F.softmax(preds, dim=1)\n    true_1_hot = true_1_hot.type(preds.type()).contiguous()\n    dims = (0,) + tuple(range(2, targets.ndimension()))\n    intersection = torch.sum(probas * true_1_hot, dims)\n    cardinality = torch.sum(probas + true_1_hot, dims)\n    union = cardinality - intersection\n    jacc_loss = (intersection / (union + eps)).mean()\n    return (1 - jacc_loss)\n\n\n\none_epoch( train, val )\n\ndef train_one_epoch(train_dataloader, model, optimizer, device):\n    model.train()\n    losses = [] \n    for i, (imgs, targets, _) in enumerate(train_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        preds = model(imgs)['out']     # forward \n        loss = ce_loss(preds, targets) # calculates the iteration loss  \n        optimizer.zero_grad()   # zeros the parameter gradients\n        loss.backward()         # backward\n        optimizer.step()        # update weights\n        print('\\t iteration: %d/%d, loss=%.4f' % (i, len(train_dataloader)-1, loss))    \n        losses.append(loss.item())\n    return torch.tensor(losses).mean().item()\n\n\ndef val_one_epoch(val_dataloader, model, confusion_matrix, device):\n    model.eval()\n    losses = []\n    total = 0\n    for i, (imgs, targets, img_file) in enumerate(val_dataloader):\n        imgs, targets = imgs.to(device), targets.to(device)\n        with torch.no_grad():\n            preds = model(imgs)['out']   # forward, preds: (B, 2, H, W)\n            loss = ce_loss(preds, targets)\n            losses.append(loss.item())\n            confusion_matrix.process_batch(preds, targets)\n            total += preds.size(0)\n            # sample images\n            if i == 0:\n                preds = torch.argmax(preds, axis=1) # (1, H, W)  \n                for j in range(3):\n                    save_file = os.path.join('outputs', 'val_%d.png' % (j))\n                    plot_image(imgs[j], preds[j], save_file)\n                \n    avg_loss = torch.tensor(losses).mean().item()\n    \n    return avg_loss\n\n\n\nTrain\n\ndef train(epochs=200, batch_size = 8 , name = 'suwany'):\n    # wandb settings\n    wandb.init(id=name, resume='allow', mode='disabled')\n    wandb.config.update({\n        'epochs': epochs,\n        'batch_size': batch_size,\n        'name': name\n    })\n    \n    # Train dataset\n    train_dataset = KariRoadDataset('./data/kari-road', train=True)\n    # Train dataloader\n    num_workers = min([os.cpu_count(), batch_size, 16])\n    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n\n    # Validation dataset\n    val_dataset = KariRoadDataset('./data/kari-road', train=False)\n    val_dataloader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, \n                            shuffle=True, num_workers=num_workers, pin_memory=True, drop_last=False)\n    \n    # Network model\n    num_classes = 10 # background + 1 classes\n    model = models.segmentation.deeplabv3_resnet101(num_classes=num_classes)  \n    \n    # GPU-support\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    if torch.cuda.device_count() &gt; 1:   # multi-GPU\n        model = torch.nn.DataParallel(model)\n    model.to(device)\n\n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=3e-4)\n      \n    # Learning rate scheduler\n    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n    # loading a weight file (if exists)\n    weight_file = Path('weights')/(name + '.pth')\n    best_accuracy = 0.0\n    start_epoch, end_epoch = (0, epochs)\n    if os.path.exists(weight_file):\n        checkpoint = torch.load(weight_file)\n        model.load_state_dict(checkpoint['model'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_accuracy = checkpoint['best_accuracy']\n        print('resumed from epoch %d' % start_epoch)\n    \n    \n    confusion_matrix = ConfusionMatrix(num_classes)\n # training/validation\n    for epoch in range(start_epoch, end_epoch):\n        print('epoch: %d/%d' % (epoch, end_epoch-1))\n        t0 = time.time()\n        # training\n        epoch_loss = train_one_epoch(train_dataloader, model, optimizer, device)\n        t1 = time.time()\n        print('loss=%.4f (took %.2f sec)' % (epoch_loss, t1-t0))\n        lr_scheduler.step(epoch_loss)\n        # validation\n        val_epoch_loss = val_one_epoch(val_dataloader, model, confusion_matrix, device)\n        val_epoch_iou = confusion_matrix.get_iou()\n        val_epoch_mean_iou = confusion_matrix.get_mean_iou()\n        val_epoch_pix_accuracy = confusion_matrix.get_pix_acc()\n        \n        print('[validation] loss=%.4f, mean iou=%.4f, pixel accuracy=%.4f' % \n              (val_epoch_loss, val_epoch_mean_iou, val_epoch_pix_accuracy))\n        print('class IoU: [' + ', '.join([('%.4f' % (x)) for x in val_epoch_iou]) + ']')\n        # saving the best status into a weight file\n        if val_epoch_pix_accuracy &gt; best_accuracy:\n             best_weight_file = Path('weights')/(name + '_best.pth')\n             best_accuracy = val_epoch_pix_accuracy\n             state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n             torch.save(state, best_weight_file)\n             print('best accuracy=&gt;saved\\n')\n        # saving the current status into a weight file\n        state = {'model': model.state_dict(), 'epoch': epoch, 'best_accuracy': best_accuracy}\n        torch.save(state, weight_file)\n        # wandb logging\n        wandb.log({'train_loss': epoch_loss, 'val_loss': val_epoch_loss, 'val_accuracy': val_epoch_pix_accuracy})\n        \n\n\n\n실행 코드\n\ntrain(epochs=200, batch_size = 8 , name = 'suwany')\n\nepoch: 0/199\n\n\nERROR:tornado.general:SEND Error: Host unreachable"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "semantic-segmentation",
    "section": "",
    "text": "개요\nImage semantic-segmentation using kari-dataset The goal of this project is to label every pixel in an image, dividing it into meaningful segments.\n\nThis work also references code from Dr. Ohhan.\n목차\n\n\n\n\n\n\n\n\n\n\nDate\n\n\nTitle\n\n\nAuthor\n\n\n\n\n\n\nJun 25, 2024\n\n\nRoad_segmantation\n\n\n \n\n\n\n\n\nNo matching items"
  }
]